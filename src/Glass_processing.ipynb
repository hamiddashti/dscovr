{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Proj, transform\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import h5py\n",
    "from pyhdf.SD import SD, SDC\n",
    "from osgeo import gdal, osr\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "def latlon_to_modis_tile(lat, lon):\n",
    "    # MODIS Sinusoidal Projection\n",
    "    modis_sinu = Proj(\"+proj=sinu +R=6371007.181 +nadgrids=@null +wktext\")\n",
    "    wgs84 = Proj(proj=\"latlong\", datum=\"WGS84\")\n",
    "\n",
    "    # Convert lat/lon to MODIS Sinusoidal coordinates\n",
    "    x, y = transform(wgs84, modis_sinu, lon, lat)\n",
    "\n",
    "    # MODIS Grid specifics\n",
    "    tile_size_meters = 1111950.5196666666  # Size of each MODIS tile in meters\n",
    "    x_origin = -20015109.354  # Westernmost coordinate\n",
    "    y_origin = 10007554.677  # Northernmost coordinate\n",
    "\n",
    "    h = int((x - x_origin) / tile_size_meters)\n",
    "    v = int((y_origin - y) / tile_size_meters)\n",
    "\n",
    "    return h, v\n",
    "\n",
    "\n",
    "def generate_filename(lat, lon, template=\"GLASS01E01.V60.A2002001.h00v08.2022010.hdf\"):\n",
    "    h, v = latlon_to_modis_tile(lat, lon)\n",
    "    hv_str = f\"h{h:02d}v{v:02d}\"\n",
    "    filename = template.replace(\"h00v08\", hv_str)\n",
    "    return filename\n",
    "\n",
    "\n",
    "def get_hdf_files(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        hdf_files = [\n",
    "            a[\"href\"]\n",
    "            for a in soup.find_all(\"a\", href=True)\n",
    "            if a[\"href\"].endswith(\".hdf\")\n",
    "        ]\n",
    "        return hdf_files\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def filter_files_by_tile(hdf_files, h, v):\n",
    "    hv_str = f\"h{h:02d}v{v:02d}\"\n",
    "    filtered_files = [file for file in hdf_files if hv_str in file]\n",
    "    return filtered_files\n",
    "\n",
    "\n",
    "def download_file(url, file_name, output_directory):\n",
    "    file_url = url + file_name\n",
    "    response = requests.get(file_url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        file_path = os.path.join(output_directory, file_name)\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        # print(f\"Downloaded file: {file_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
    "\n",
    "\n",
    "def inspect_hdf_file(file_path):\n",
    "    with h5py.File(file_path, \"r\") as file:\n",
    "        # List all groups\n",
    "        print(\"Keys: %s\" % file.keys())\n",
    "        for key in file.keys():\n",
    "            print(f\"\\nContent in '{key}':\")\n",
    "            data = file[key]\n",
    "            print(data)\n",
    "            if isinstance(data, h5py.Dataset):\n",
    "                print(f\"Dataset '{key}' shape: {data.shape}\")\n",
    "                print(f\"Dataset '{key}' dtype: {data.dtype}\")\n",
    "                print(data[:])\n",
    "            elif isinstance(data, h5py.Group):\n",
    "                print(f\"Group '{key}' contains: {list(data.keys())}\")\n",
    "\n",
    "\n",
    "def get_directories(url):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Parse the response text with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find all the <a> tags and get their href attributes\n",
    "        links = [a[\"href\"] for a in soup.find_all(\"a\", href=True)]\n",
    "\n",
    "        # Filter out the links that don't represent directories\n",
    "        directories = [link for link in links if link.endswith(\"/\")]\n",
    "\n",
    "        return directories\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "def convert_projection(file_path):\n",
    "    try:\n",
    "        # Access the LAI_500M subdataset\n",
    "        lai_sds_path = 'HDF4_EOS:EOS_GRID:\"' + file_path + '\":GLASS01E01:LAI_500M'\n",
    "        lai_sds_ds = gdal.Open(lai_sds_path, gdal.GA_ReadOnly)\n",
    "\n",
    "        # Create a new geotransform and spatial reference for the geographic projection\n",
    "        srs = osr.SpatialReference()\n",
    "        srs.ImportFromEPSG(4326)  # WGS84\n",
    "\n",
    "        # Create the output filename\n",
    "        output_file = os.path.splitext(file_path)[0] + \"_geographic.tif\"\n",
    "\n",
    "        # Use gdal.Warp to convert the projection\n",
    "        gdal.Warp(output_file, lai_sds_ds, dstSRS=srs)\n",
    "\n",
    "        # print(f\"File saved at: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AR-TF1</td>\n",
       "      <td>-54.9733</td>\n",
       "      <td>-66.7335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BR-CST</td>\n",
       "      <td>-7.9682</td>\n",
       "      <td>-38.3842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BR-Npw</td>\n",
       "      <td>-16.4980</td>\n",
       "      <td>-56.4120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CA-ARB</td>\n",
       "      <td>52.6950</td>\n",
       "      <td>-83.9452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA-ARF</td>\n",
       "      <td>52.7008</td>\n",
       "      <td>-83.9550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>US-Uaf</td>\n",
       "      <td>64.8663</td>\n",
       "      <td>-147.8555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>US-WCr</td>\n",
       "      <td>45.8059</td>\n",
       "      <td>-90.0799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>US-Wi2</td>\n",
       "      <td>46.6869</td>\n",
       "      <td>-91.1528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>ZA-Kru</td>\n",
       "      <td>-25.0197</td>\n",
       "      <td>31.4969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>ZM-Mon</td>\n",
       "      <td>-15.4391</td>\n",
       "      <td>23.2525</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>387 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       name      Lat       Lon\n",
       "0    AR-TF1 -54.9733  -66.7335\n",
       "1    BR-CST  -7.9682  -38.3842\n",
       "2    BR-Npw -16.4980  -56.4120\n",
       "3    CA-ARB  52.6950  -83.9452\n",
       "4    CA-ARF  52.7008  -83.9550\n",
       "..      ...      ...       ...\n",
       "382  US-Uaf  64.8663 -147.8555\n",
       "383  US-WCr  45.8059  -90.0799\n",
       "384  US-Wi2  46.6869  -91.1528\n",
       "385  ZA-Kru -25.0197   31.4969\n",
       "386  ZM-Mon -15.4391   23.2525\n",
       "\n",
       "[387 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ameriflux_coords = pd.read_csv(\"../data/Ameriflux_coords.csv\")\n",
    "fluxnet_coords = pd.read_csv(\"../data/Fluxnet_coords.csv\")\n",
    "merged_coords = pd.concat([ameriflux_coords, fluxnet_coords], ignore_index=True)\n",
    "merged_coords.drop_duplicates(subset=merged_coords.columns[0], inplace=True)\n",
    "merged_coords.reset_index(drop=True, inplace=True)\n",
    "merged_coords.rename({\"Name\": \"name\"}, axis=1, inplace=True)\n",
    "merged_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0, name AR-TF1, year 2002, directory 001/\n",
      "index 0, name AR-TF1, year 2002, directory 009/\n",
      "index 0, name AR-TF1, year 2002, directory 017/\n",
      "index 0, name AR-TF1, year 2002, directory 025/\n",
      "index 0, name AR-TF1, year 2002, directory 033/\n",
      "index 0, name AR-TF1, year 2002, directory 041/\n",
      "index 0, name AR-TF1, year 2002, directory 049/\n",
      "index 0, name AR-TF1, year 2002, directory 057/\n",
      "index 0, name AR-TF1, year 2002, directory 065/\n",
      "index 0, name AR-TF1, year 2003, directory 001/\n",
      "index 0, name AR-TF1, year 2003, directory 009/\n",
      "index 0, name AR-TF1, year 2003, directory 017/\n",
      "index 0, name AR-TF1, year 2003, directory 025/\n",
      "index 0, name AR-TF1, year 2003, directory 033/\n",
      "index 0, name AR-TF1, year 2003, directory 041/\n",
      "index 0, name AR-TF1, year 2003, directory 049/\n",
      "index 0, name AR-TF1, year 2003, directory 057/\n",
      "index 0, name AR-TF1, year 2003, directory 065/\n",
      "index 1, name BR-CST, year 2002, directory 001/\n",
      "index 1, name BR-CST, year 2002, directory 009/\n",
      "index 1, name BR-CST, year 2002, directory 017/\n",
      "index 1, name BR-CST, year 2002, directory 025/\n",
      "index 1, name BR-CST, year 2002, directory 033/\n",
      "index 1, name BR-CST, year 2002, directory 041/\n",
      "index 1, name BR-CST, year 2002, directory 049/\n",
      "index 1, name BR-CST, year 2002, directory 057/\n",
      "index 1, name BR-CST, year 2002, directory 065/\n",
      "index 1, name BR-CST, year 2003, directory 001/\n",
      "index 1, name BR-CST, year 2003, directory 009/\n",
      "index 1, name BR-CST, year 2003, directory 017/\n",
      "index 1, name BR-CST, year 2003, directory 025/\n",
      "index 1, name BR-CST, year 2003, directory 033/\n",
      "index 1, name BR-CST, year 2003, directory 041/\n",
      "index 1, name BR-CST, year 2003, directory 049/\n",
      "index 1, name BR-CST, year 2003, directory 057/\n",
      "index 1, name BR-CST, year 2003, directory 065/\n"
     ]
    }
   ],
   "source": [
    "site_names = merged_coords[\"name\"]\n",
    "site_lat = merged_coords[\"Lat\"]\n",
    "site_lon = merged_coords[\"Lon\"]\n",
    "\n",
    "for i in range(0, len(site_names)):\n",
    "    name = site_names[i]\n",
    "    lat = site_lat[i]\n",
    "    lon = site_lon[i]\n",
    "    for year in range(2002, 2021):\n",
    "        output_directory = f\"../outputs/Glass/{name}/{str(year)}\"\n",
    "        if not os.path.exists(output_directory):\n",
    "            os.makedirs(output_directory)\n",
    "        directories = get_directories(base_url + str(year) + \"/\")\n",
    "        for j in range(1, len(directories)):\n",
    "            url = base_url + str(year) + \"/\" + directories[j]\n",
    "            h, v = latlon_to_modis_tile(lat, lon)\n",
    "            print(f\"index {i}, name {name}, year {year}, directory {directories[j]}\")\n",
    "            hdf_files = get_hdf_files(url)\n",
    "            filtered_files = filter_files_by_tile(hdf_files, h, v)\n",
    "            if filtered_files:\n",
    "                file_to_download = filtered_files[0]\n",
    "                download_file(url, file_to_download, output_directory)\n",
    "                file_path = f\"../outputs/Glass/{name}/{str(year)}/\" + filtered_files[0]\n",
    "                convert_projection(file_path)\n",
    "                os.remove(file_path)\n",
    "            else:\n",
    "                print(\n",
    "                    f\"No .hdf files found for MODIS, name {name}, year {year}, directory {directories[j]}.\"\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dscovr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
