{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from scipy.stats import zscore, linregress\n",
    "from pandas.plotting import scatter_matrix\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def remove_outliers(df, threshold=2):\n",
    "    # Initialize a boolean mask to keep track of rows to drop\n",
    "    outlier_rows_mask = np.zeros(len(df), dtype=bool)\n",
    "\n",
    "    # Iterate over each column\n",
    "    for col in df.columns:\n",
    "        # Skip the \"t1\" and \"t2\" columns\n",
    "        if col == \"t1\" or col == \"t2\":\n",
    "            continue\n",
    "\n",
    "        # Calculate the mean and standard deviation of the column\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "\n",
    "        # Find outliers in this column\n",
    "        outliers = (df[col] - mean).abs() > threshold * std\n",
    "\n",
    "        # Mark rows with outliers in this column\n",
    "        outlier_rows_mask = np.logical_or(outlier_rows_mask, outliers)\n",
    "\n",
    "    # Drop rows with outliers\n",
    "    cleaned_df = df[~outlier_rows_mask]\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluxnet_info = pd.read_csv(\"../data/EC/fluxnet/sites_info.csv\")\n",
    "ameriflux_info = pd.read_csv(\"../data/EC/Ameriflux/sites_info.tsv\", delimiter=\"\\t\")\n",
    "fluxnet_names = fluxnet_info[\"ID\"].to_list()\n",
    "fluxnet_types = fluxnet_info[\"type\"].to_list()\n",
    "ameriflux_names = ameriflux_info[\"Site ID\"].to_list()\n",
    "ameriflux_types = ameriflux_info[\"Vegetation Abbreviation (IGBP)\"].to_list()\n",
    "\n",
    "combined_names = list(set(ameriflux_names + fluxnet_names))\n",
    "combined_types = []\n",
    "for name in combined_names:\n",
    "    if name in ameriflux_names and name in fluxnet_names:\n",
    "        # Choose a type from either fluxnet_types or ameriflux_types\n",
    "        combined_types.append(fluxnet_types[fluxnet_names.index(name)])\n",
    "    elif name in ameriflux_names:\n",
    "        combined_types.append(ameriflux_types[ameriflux_names.index(name)])\n",
    "    else:\n",
    "        combined_types.append(fluxnet_types[fluxnet_names.index(name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ec = []\n",
    "\n",
    "for i in range(len(combined_names)):\n",
    "    site_name = combined_names[i]\n",
    "    site_type = combined_types[i]\n",
    "\n",
    "    if site_name in ameriflux_names:\n",
    "        file = glob.glob(\"../data/EC/Ameriflux/AMF_\" + site_name + \"*DD*\")\n",
    "    else:\n",
    "        file = glob.glob(\"../data/EC/fluxnet/FLX_\" + site_name + \"*DD*\")\n",
    "\n",
    "    ec = pd.read_csv(file[0])\n",
    "    ec.loc[:, \"type\"] = site_type\n",
    "    ec.loc[:, \"name\"] = site_name\n",
    "    ec.index = pd.to_datetime(ec[\"TIMESTAMP\"], format=\"%Y%m%d\")\n",
    "    combined_ec.append(ec)\n",
    "combined_ec = pd.concat(combined_ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCD43_fluxnet = []\n",
    "MCD15_fluxnet = []\n",
    "MCD43_ameriflux = []\n",
    "MCD15_ameriflux = []\n",
    "\n",
    "# Loop over batches (#5) of downloaded data\n",
    "for i in range(1, 5):\n",
    "    refl_fluxnet = glob.glob(\n",
    "        \"../data/EC/fluxnet/sat_data/*batch\" + str(i) + \"*MCD43A4-061-results.csv\"\n",
    "    )\n",
    "    sat_refl_fluxnet = pd.read_csv(refl_fluxnet[0])\n",
    "    sat_refl_fluxnet.loc[:, \"time\"] = pd.to_datetime(sat_refl_fluxnet[\"Date\"])\n",
    "    sat_refl_fluxnet.set_index(sat_refl_fluxnet[\"Date\"], inplace=True)\n",
    "    MCD43_fluxnet.append(sat_refl_fluxnet)\n",
    "\n",
    "    fpar_fluxnet = glob.glob(\n",
    "        \"../data/EC/fluxnet/sat_data/*batch\" + str(i) + \"*MCD15A3H-061-results.csv\"\n",
    "    )\n",
    "    sat_fpar_fluxnet = pd.read_csv(fpar_fluxnet[0])\n",
    "    sat_fpar_fluxnet.loc[:, \"time\"] = pd.to_datetime(sat_fpar_fluxnet[\"Date\"])\n",
    "    sat_fpar_fluxnet.set_index(sat_fpar_fluxnet[\"Date\"], inplace=True)\n",
    "    MCD15_fluxnet.append(sat_fpar_fluxnet)\n",
    "\n",
    "    if i < 5:\n",
    "        refl_ameriflux = glob.glob(\n",
    "            \"../data/EC/Ameriflux/sat_data/*batch\" + str(i) + \"*MCD43A4-061-results.csv\"\n",
    "        )\n",
    "\n",
    "        sat_refl_ameriflux = pd.read_csv(refl_ameriflux[0])\n",
    "        sat_refl_ameriflux.loc[:, \"time\"] = pd.to_datetime(sat_refl_ameriflux[\"Date\"])\n",
    "        sat_refl_ameriflux.set_index(sat_refl_ameriflux[\"Date\"], inplace=True)\n",
    "        MCD43_ameriflux.append(sat_refl_ameriflux)\n",
    "\n",
    "        fpar_ameriflux = glob.glob(\n",
    "            \"../data/EC/Ameriflux/sat_data/*batch\"\n",
    "            + str(i)\n",
    "            + \"*MCD15A3H-061-results.csv\"\n",
    "        )\n",
    "        fpar_ameriflux = pd.read_csv(fpar_ameriflux[0])\n",
    "\n",
    "        fpar_ameriflux.loc[:, \"time\"] = pd.to_datetime(fpar_ameriflux[\"Date\"])\n",
    "        fpar_ameriflux.set_index(fpar_ameriflux[\"Date\"], inplace=True)\n",
    "        MCD15_ameriflux.append(fpar_ameriflux)\n",
    "\n",
    "\n",
    "refl_fluxnet = pd.concat(MCD43_fluxnet)\n",
    "refl_fluxnet = refl_fluxnet.rename(columns={\"ID\": \"name\"})\n",
    "\n",
    "fpar_fluxnet = pd.concat(MCD15_fluxnet)\n",
    "fpar_fluxnet = fpar_fluxnet.rename(columns={\"ID\": \"name\"})\n",
    "\n",
    "refl_ameriflux = pd.concat(MCD43_ameriflux)\n",
    "refl_ameriflux = refl_ameriflux.rename(columns={\"ID\": \"name\"})\n",
    "fpar_ameriflux = pd.concat(MCD15_ameriflux)\n",
    "fpar_ameriflux = fpar_ameriflux.rename(columns={\"ID\": \"name\"})\n",
    "combined_refl = []\n",
    "combined_fpar = []\n",
    "\n",
    "for name in combined_names:\n",
    "    if name in ameriflux_names:\n",
    "        selected_refl = refl_ameriflux[refl_ameriflux[\"name\"] == name]\n",
    "        selected_fpar = fpar_ameriflux[fpar_ameriflux[\"name\"] == name]\n",
    "    else:\n",
    "        selected_refl = refl_fluxnet[refl_fluxnet[\"name\"] == name]\n",
    "        selected_fpar = fpar_fluxnet[fpar_fluxnet[\"name\"] == name]\n",
    "\n",
    "    combined_refl.append(selected_refl)\n",
    "    combined_fpar.append(selected_fpar)\n",
    "\n",
    "combined_refl = pd.concat(combined_refl)\n",
    "combined_fpar = pd.concat(combined_fpar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flags come from [This paper](https://pdf.sciencedirectassets.com/271723/1-s2.0-S0168192320X00062/1-s2.0-S0168192320301945/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDsaCXVzLWVhc3QtMSJGMEQCIH4OB25%2B%2BrWEsBRZZ4CMPUgYETlwWwu0kcC2JF5q5VHKAiAysHZntlDvw8izOPtHcQa1YDDXXdQ%2ByZrXPCaxn2NdLiqzBQg0EAUaDDA1OTAwMzU0Njg2NSIMhywMz6eScMTpU2DpKpAFhCNp4JUVE3sxgm2Ly4FDKc%2BSr7igdIpyk9TsBrE%2F0lC51AZ4N3waj2MpXB%2BA2dorxigw%2BZqxNIgin%2BQFdFvFUN0k6mgrcLTBkS9FhRXOMtTKHvqFgR6FAHKZynNnFuxkIM6eV3dZLRHS0R2yeyRpHxGUk%2FYdd6MCozWZKdmaO00mNuMaCQNgwifIIBvwKStqkc9WTys%2F0PXrBO48pSfm90AcEbBzjGiJRgmIpoKoW%2BUkwvBmd%2ByoyK9%2FIQ8nHz9nN7g%2FgopA5nBLXDRvT29mo0D3nFjO6of%2Fm0aPVA0cX2OCmbYDrdb4s%2BTl%2Fb3Cx0HBquT78mhkYTCbOd6YxFHpmb1s6QDcD%2B%2Bl003yqSDYyJdzZmoq0D8wI8NQWJcycDEPI5fgCYxaS0WubAL2QPYO4u%2BOopqWqBfe1l4gf7nkEe8Pp5UAe2vjYGxV5mN9dkOOhlrKXpWa697KXIATqSoYGQwrkumPECnPQWs3FFVrwDiwWDJNvUqUikDqIFG1zFGD7xwzLQh%2BWfrKyxhi%2BfiE8rY4YlFVQ2e6M4DwrmwPtkagsRtzufygm1nJ8bF9z%2F3aiyioMBwCeQ9y3zWT42L5V%2BRtf95N0aGjjjlfzKMB04euz6YWqkhoVuaKGo2WFlt82S4B9PPxa8d8v3bTwxZgfY%2FAXJaAsbET7i9t89h9XF9aZmjR06YbPLi4F0%2FMAbq0njAWnW6oSW9xYXl7Rugx1p8DpKKtzyBr%2FpzNj9djuFAfKUnfLOuiu8rZpsPctz%2FDKwchAqjiYLWtPsTo%2FuyIhZoDBcO3KujlTP7F4VbI87xj5JFXwdsw5C1ZrP0b8N9JZZsZO0V9yhUS8n07bIc3grqIS%2Fn7wzsBGvHu5jiqpMIwz8SIrwY6sgHXwF3s5SpzUmT%2Bd%2FpaOxFaSWK69qbq1fh%2Bv92O3hmad2L%2F82iL5dKCVYcfn5cEbl1mCCJGLbj%2B1JACCSn%2Bb0IDzbqm9PlhBh09lYsoQFVr%2FMaOCh2jlc6UJX%2BWuWuVQ1DercdbawxGlkLwfrDweiqdswRdEHNP5yU6LkedJVvBuG%2Ffm4Kl05DMaCUpKB5zH0b04M16ASPcw5wiPUpNMuyudj1Ko4wwx2sOZt%2BDxbVJHEdP&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240301T194332Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYY7GY5ACN%2F20240301%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=746656b69aa01f343be150d203fa4d604f6da3aad6570344a69b5eda22f1ac03&hash=da2637ff2882758ae7d5d5d82b08ed3897533c9ea8fae448ed1b18c618c8b49a&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0168192320301945&tid=spdf-f3eb7708-9f66-472e-82f4-ba59d2040f74&sid=54e759056934f749ce7937c-05135ec4bd54gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0f1557550402015c085500&rr=85dba2f5b9a06a41&cc=us)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamid/anaconda3/envs/dscovr/lib/python3.12/site-packages/pandas/core/nanops.py:1010: RuntimeWarning: invalid value encountered in subtract\n",
      "  sqr = _ensure_numeric((avg - values) ** 2)\n"
     ]
    }
   ],
   "source": [
    "combined_data = []\n",
    "for i in range(len(combined_names)):\n",
    "    name = combined_names[i]\n",
    "    type = combined_types[i]\n",
    "    ec_data = combined_ec[combined_ec[\"name\"] == name]\n",
    "\n",
    "    # -------------Filter GPP------------------------\n",
    "    # The gap filled data is less than 20%\n",
    "    ec_data = ec_data[ec_data[\"NEE_VUT_REF_QC\"] > 0.8]\n",
    "\n",
    "    # Uncertaninity in NEE is less than 3  C m−2 d−1\n",
    "    ec_data = ec_data[ec_data[\"NEE_VUT_REF_RANDUNC\"] < 3]\n",
    "\n",
    "    # Difference between GPP day and night is less than 3 C m−2 d−1\n",
    "    idx_good_gpp = abs(ec_data[\"GPP_DT_VUT_REF\"] - ec_data[\"GPP_NT_VUT_REF\"]) < 3\n",
    "    gpp_day = ec_data[\"GPP_DT_VUT_REF\"][idx_good_gpp]\n",
    "    gpp_night = ec_data[\"GPP_NT_VUT_REF\"][idx_good_gpp]\n",
    "\n",
    "    # Take the mean of the day and night GPP\n",
    "    gpp = (gpp_day + gpp_night) / 2\n",
    "    # gpp = gpp_night\n",
    "    # Make sure there is no negative GPP\n",
    "    gpp = gpp[gpp > 0]\n",
    "\n",
    "    # ----------------Filter PAR-----------------------\n",
    "    par_qc = ec_data.loc[gpp.index, \"PPFD_IN_QC\"]\n",
    "    idx_good_par = par_qc[par_qc > 0.8]\n",
    "    par = ec_data.loc[idx_good_par.index, \"PPFD_IN\"]\n",
    "    gpp = gpp[par.index]\n",
    "    gpp = gpp.to_frame(\"gpp\")\n",
    "    par = par.to_frame(\"par\")\n",
    "    site_ec = pd.concat([gpp, par], axis=1)\n",
    "\n",
    "    # ----------------Filter Reflectance and FPAR-----------------------\n",
    "    site_refl = combined_refl[combined_refl[\"name\"] == name]\n",
    "    # site_refl = site_refl[site_refl.index.isin(gpp.index)]\n",
    "    site_refl.index = pd.to_datetime(site_refl.index, format=\"%Y-%m-%d\")\n",
    "    site_refl = site_refl[site_refl.index.isin(site_ec.index)]\n",
    "    filtered_refl = site_refl[\n",
    "        (\n",
    "            site_refl[\"MCD43A4_061_BRDF_Albedo_Band_Mandatory_Quality_Band1_MODLAND\"]\n",
    "            == \"0b000\"\n",
    "        )\n",
    "        & (\n",
    "            site_refl[\"MCD43A4_061_BRDF_Albedo_Band_Mandatory_Quality_Band2_MODLAND\"]\n",
    "            == \"0b000\"\n",
    "        )\n",
    "    ].copy()\n",
    "\n",
    "    site_red = filtered_refl[[\"MCD43A4_061_Nadir_Reflectance_Band1\"]].rename(\n",
    "        columns={\"MCD43A4_061_Nadir_Reflectance_Band1\": \"red\"}\n",
    "    )\n",
    "    site_nir = filtered_refl[[\"MCD43A4_061_Nadir_Reflectance_Band2\"]].rename(\n",
    "        columns={\"MCD43A4_061_Nadir_Reflectance_Band2\": \"nir\"}\n",
    "    )\n",
    "\n",
    "    site_fpar = combined_fpar[combined_fpar[\"name\"] == \"US-UMB\"]\n",
    "    site_fpar.index = pd.to_datetime(site_fpar.index, format=\"%Y-%m-%d\")\n",
    "\n",
    "    filtered_fpar = site_fpar[\n",
    "        (site_fpar[\"MCD15A3H_061_FparLai_QC_MODLAND\"] == \"0b0\")\n",
    "        & (site_fpar[\"MCD15A3H_061_FparLai_QC_DeadDetector\"] == \"0b0\")\n",
    "        & (site_fpar[\"MCD15A3H_061_FparLai_QC_CloudState\"] == \"0b00\")\n",
    "        & (site_fpar[\"MCD15A3H_061_FparLai_QC_SCF_QC\"].isin([\"0b000\", \"0b001\"]))\n",
    "    ].copy()\n",
    "    if filtered_fpar.empty:\n",
    "        continue\n",
    "    fpar_tmp = filtered_fpar[\"MCD15A3H_061_Fpar_500m\"]\n",
    "    site_fpar = fpar_tmp.resample(\"D\").interpolate(\"linear\")\n",
    "    site_fpar = site_fpar.to_frame(\"fpar\")\n",
    "    site_fpar = site_fpar[site_fpar.index.isin(site_ec.index)]\n",
    "    # Merge the dataframes\n",
    "    site_df = (\n",
    "        site_ec.merge(site_red, left_index=True, right_index=True)\n",
    "        .merge(site_nir, left_index=True, right_index=True)\n",
    "        .merge(site_fpar, left_index=True, right_index=True)\n",
    "    )\n",
    "    if site_df.empty:\n",
    "        continue\n",
    "    # Calculate the NDVI, NIRv, NIRvp, Fesc, and LUE\n",
    "    site_df.loc[:, \"ndvi\"] = (site_df[\"nir\"] - site_df[\"red\"]) / (\n",
    "        site_df[\"nir\"] + site_df[\"red\"]\n",
    "    )\n",
    "    site_df.loc[:, \"nirv\"] = site_df[\"ndvi\"] * site_df[\"nir\"]\n",
    "    site_df.loc[:, \"nirvp\"] = site_df[\"nirv\"] * site_df[\"par\"]\n",
    "    site_df.loc[:, \"fesc\"] = site_df[\"nirv\"] / site_df[\"fpar\"]\n",
    "    site_df.loc[:, \"lue\"] = site_df[\"gpp\"] / (site_df[\"par\"] * site_df[\"fpar\"])\n",
    "    cleaned_site_df = remove_outliers(site_df).copy()\n",
    "    cleaned_site_df.loc[:, \"name\"] = name\n",
    "    cleaned_site_df.loc[:, \"type\"] = type\n",
    "    combined_data.append(cleaned_site_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(combined_data)\n",
    "names = df[\"name\"].unique()\n",
    "df.to_csv(\"../outputs/data_clean.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dscovr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
