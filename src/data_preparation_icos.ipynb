{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from scipy.stats import zscore, linregress\n",
    "from pandas.plotting import scatter_matrix\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "def remove_outliers(df, threshold=3):\n",
    "    # Initialize a boolean mask to keep track of rows to drop\n",
    "    outlier_rows_mask = np.zeros(len(df), dtype=bool)\n",
    "\n",
    "    # Iterate over each column\n",
    "    for col in df.columns:\n",
    "        # Skip the \"t1\" and \"t2\" columns\n",
    "        if col == \"t1\" or col == \"t2\":\n",
    "            continue\n",
    "\n",
    "        # Calculate the mean and standard deviation of the column\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "\n",
    "        # Find outliers in this column\n",
    "        outliers = (df[col] - mean).abs() > threshold * std\n",
    "\n",
    "        # Mark rows with outliers in this column\n",
    "        outlier_rows_mask = np.logical_or(outlier_rows_mask, outliers)\n",
    "\n",
    "    # Drop rows with outliers\n",
    "    cleaned_df = df[~outlier_rows_mask]\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data and make them consistent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluxnet_info = pd.read_csv(\"../data/EC/fluxnet/sites_info.csv\")\n",
    "ameriflux_info = pd.read_csv(\"../data/EC/Ameriflux/sites_info.tsv\", delimiter=\"\\t\")\n",
    "Icos_info = pd.read_csv(\"../data/EC/ICOS/sites_info.csv\")\n",
    "\n",
    "fluxnet_names = fluxnet_info[\"ID\"].to_list()\n",
    "fluxnet_types = fluxnet_info[\"type\"].to_list()\n",
    "\n",
    "ameriflux_names = ameriflux_info[\"Site ID\"].to_list()\n",
    "ameriflux_types = ameriflux_info[\"Vegetation Abbreviation (IGBP)\"].to_list()\n",
    "\n",
    "icos_names = Icos_info[\"ID\"].to_list()\n",
    "icos_types = Icos_info[\"type\"].to_list()\n",
    "icos_id = Icos_info[\"id_number\"].to_list()\n",
    "icos_dict = dict(zip(icos_id, icos_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_names = list(set(ameriflux_names + fluxnet_names + icos_names))\n",
    "combined_types = []\n",
    "for name in combined_names:\n",
    "    if name in ameriflux_names and name in fluxnet_names:\n",
    "        # Choose a type from either fluxnet_types or ameriflux_types\n",
    "        combined_types.append(fluxnet_types[fluxnet_names.index(name)])\n",
    "    elif name in ameriflux_names:\n",
    "        combined_types.append(ameriflux_types[ameriflux_names.index(name)])\n",
    "    elif name in icos_names:\n",
    "        combined_types.append(icos_types[icos_names.index(name)])\n",
    "    else:\n",
    "        combined_types.append(fluxnet_types[fluxnet_names.index(name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ec = []\n",
    "\n",
    "for i in range(len(combined_names)):\n",
    "    site_name = combined_names[i]\n",
    "    site_type = combined_types[i]\n",
    "\n",
    "    if site_name in ameriflux_names:\n",
    "        file = glob.glob(\"../data/EC/Ameriflux/AMF_\" + site_name + \"*DD*\")\n",
    "    elif site_name in icos_names:\n",
    "        file = glob.glob(\n",
    "            \"../data/EC/ICOS/Data/FLX_\" + site_name + \"*\" + \"/*FULLSET_DD*\"\n",
    "        )\n",
    "    else:\n",
    "        file = glob.glob(\"../data/EC/fluxnet/FLX_\" + site_name + \"*DD*\")\n",
    "\n",
    "    ec = pd.read_csv(file[0])\n",
    "    ec.loc[:, \"type\"] = site_type\n",
    "    ec.loc[:, \"name\"] = site_name\n",
    "    ec.index = pd.to_datetime(ec[\"TIMESTAMP\"], format=\"%Y%m%d\")\n",
    "    combined_ec.append(ec)\n",
    "\n",
    "combined_ec = pd.concat(combined_ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for five day fpar\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "MCD43_fluxnet = []\n",
    "MCD43_ameriflux = []\n",
    "MCD43_icos = []\n",
    "\n",
    "MCD15_fluxnet = []\n",
    "MCD15_ameriflux = []\n",
    "MCD15_icos = []\n",
    "\n",
    "# Loop over batches (#5) of downloaded data\n",
    "for i in range(1, 6):\n",
    "    refl_fluxnet = glob.glob(\n",
    "        \"../data/EC/fluxnet/sat_data/*batch\" + str(i) + \"*MCD43A4-061-results.csv\"\n",
    "    )\n",
    "    sat_refl_fluxnet = pd.read_csv(refl_fluxnet[0])\n",
    "    sat_refl_fluxnet.loc[:, \"time\"] = pd.to_datetime(sat_refl_fluxnet[\"Date\"])\n",
    "    sat_refl_fluxnet.set_index(sat_refl_fluxnet[\"Date\"], inplace=True)\n",
    "    MCD43_fluxnet.append(sat_refl_fluxnet)\n",
    "\n",
    "    fpar_fluxnet = glob.glob(\n",
    "        \"../data/EC/fluxnet/sat_data/*batch\" + str(i) + \"*MCD15A3H-061-results.csv\"\n",
    "    )\n",
    "    sat_fpar_fluxnet = pd.read_csv(fpar_fluxnet[0])\n",
    "    sat_fpar_fluxnet.loc[:, \"time\"] = pd.to_datetime(sat_fpar_fluxnet[\"Date\"])\n",
    "    sat_fpar_fluxnet.set_index(sat_fpar_fluxnet[\"Date\"], inplace=True)\n",
    "    MCD15_fluxnet.append(sat_fpar_fluxnet)\n",
    "\n",
    "    # Note all the ameriflux sites are in 4 batches\n",
    "    if i < 5:\n",
    "        refl_ameriflux = glob.glob(\n",
    "            \"../data/EC/Ameriflux/sat_data/*batch\" + str(i) + \"*MCD43A4-061-results.csv\"\n",
    "        )\n",
    "\n",
    "        sat_refl_ameriflux = pd.read_csv(refl_ameriflux[0])\n",
    "        sat_refl_ameriflux.loc[:, \"time\"] = pd.to_datetime(sat_refl_ameriflux[\"Date\"])\n",
    "        sat_refl_ameriflux.set_index(sat_refl_ameriflux[\"Date\"], inplace=True)\n",
    "        MCD43_ameriflux.append(sat_refl_ameriflux)\n",
    "\n",
    "        fpar_ameriflux = glob.glob(\n",
    "            \"../data/EC/Ameriflux/sat_data/*batch\"\n",
    "            + str(i)\n",
    "            + \"*MCD15A3H-061-results.csv\"\n",
    "        )\n",
    "        sat_fpar_ameriflux = pd.read_csv(fpar_ameriflux[0])  # Changed variable name\n",
    "\n",
    "        sat_fpar_ameriflux.loc[:, \"time\"] = pd.to_datetime(sat_fpar_ameriflux[\"Date\"])\n",
    "        sat_fpar_ameriflux.set_index(sat_fpar_ameriflux[\"Date\"], inplace=True)\n",
    "        MCD15_ameriflux.append(sat_fpar_ameriflux)\n",
    "    if i < 3:\n",
    "        refl_icos = glob.glob(\n",
    "            \"../data/EC/ICOS/sat_data/*batch\" + str(i) + \"*MCD43A4-061-results.csv\"\n",
    "        )\n",
    "        sat_refl_icos = pd.read_csv(refl_icos[0])\n",
    "        refl_icos = glob.glob(\n",
    "            \"../data/EC/ICOS/sat_data/*batch\" + str(i) + \"*MCD43A4-061-results.csv\"\n",
    "        )\n",
    "        sat_refl_icos = pd.read_csv(refl_icos[0])\n",
    "        sat_refl_icos[\"ID\"] = sat_refl_icos[\"ID\"].map(icos_dict)\n",
    "        sat_refl_icos.dropna(subset=[\"ID\"], inplace=True)\n",
    "        sat_refl_icos.loc[:, \"time\"] = pd.to_datetime(sat_refl_icos[\"Date\"])\n",
    "        sat_refl_icos.set_index(sat_refl_icos[\"Date\"], inplace=True)\n",
    "        MCD43_icos.append(sat_refl_icos)\n",
    "\n",
    "        fpar_icos = glob.glob(\n",
    "            \"../data/EC/ICOS/sat_data/*batch\" + str(i) + \"*MCD15A3H-061-results.csv\"\n",
    "        )\n",
    "        sat_fpar_icos = pd.read_csv(fpar_icos[0])\n",
    "        sat_fpar_icos[\"ID\"] = sat_fpar_icos[\"ID\"].map(icos_dict)\n",
    "        sat_fpar_icos.dropna(subset=[\"ID\"], inplace=True)\n",
    "\n",
    "        sat_fpar_icos.loc[:, \"time\"] = pd.to_datetime(sat_fpar_icos[\"Date\"])\n",
    "        sat_fpar_icos.set_index(sat_fpar_icos[\"Date\"], inplace=True)\n",
    "        MCD15_icos.append(sat_fpar_icos)\n",
    "\n",
    "\n",
    "refl_fluxnet = pd.concat(MCD43_fluxnet)\n",
    "refl_fluxnet = refl_fluxnet.rename(columns={\"ID\": \"name\"})\n",
    "fpar_fluxnet = pd.concat(MCD15_fluxnet)\n",
    "fpar_fluxnet = fpar_fluxnet.rename(columns={\"ID\": \"name\"})\n",
    "\n",
    "refl_ameriflux = pd.concat(MCD43_ameriflux)\n",
    "refl_ameriflux = refl_ameriflux.rename(columns={\"ID\": \"name\"})\n",
    "fpar_ameriflux = pd.concat(MCD15_ameriflux)\n",
    "fpar_ameriflux = fpar_ameriflux.rename(columns={\"ID\": \"name\"})\n",
    "\n",
    "refl_icos = pd.concat(MCD43_icos)\n",
    "refl_icos = refl_icos.rename(columns={\"ID\": \"name\"})\n",
    "fpar_icos = pd.concat(MCD15_icos)\n",
    "fpar_icos = fpar_icos.rename(columns={\"ID\": \"name\"})\n",
    "\n",
    "\n",
    "combined_refl = []\n",
    "combined_fpar = []\n",
    "\n",
    "for name in combined_names:\n",
    "    if name in ameriflux_names:\n",
    "        selected_refl = refl_ameriflux[refl_ameriflux[\"name\"] == name]\n",
    "        selected_fpar = fpar_ameriflux[fpar_ameriflux[\"name\"] == name]\n",
    "    elif name in icos_names:\n",
    "        selected_refl = refl_icos[refl_icos[\"name\"] == name]\n",
    "        selected_fpar = fpar_icos[fpar_icos[\"name\"] == name]\n",
    "    else:\n",
    "        selected_refl = refl_fluxnet[refl_fluxnet[\"name\"] == name]\n",
    "        selected_fpar = fpar_fluxnet[fpar_fluxnet[\"name\"] == name]\n",
    "\n",
    "    combined_refl.append(selected_refl)\n",
    "    combined_fpar.append(selected_fpar)\n",
    "\n",
    "combined_refl = pd.concat(combined_refl)\n",
    "combined_fpar = pd.concat(combined_fpar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flags come from [This paper](https://pdf.sciencedirectassets.com/271723/1-s2.0-S0168192320X00062/1-s2.0-S0168192320301945/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDsaCXVzLWVhc3QtMSJGMEQCIH4OB25%2B%2BrWEsBRZZ4CMPUgYETlwWwu0kcC2JF5q5VHKAiAysHZntlDvw8izOPtHcQa1YDDXXdQ%2ByZrXPCaxn2NdLiqzBQg0EAUaDDA1OTAwMzU0Njg2NSIMhywMz6eScMTpU2DpKpAFhCNp4JUVE3sxgm2Ly4FDKc%2BSr7igdIpyk9TsBrE%2F0lC51AZ4N3waj2MpXB%2BA2dorxigw%2BZqxNIgin%2BQFdFvFUN0k6mgrcLTBkS9FhRXOMtTKHvqFgR6FAHKZynNnFuxkIM6eV3dZLRHS0R2yeyRpHxGUk%2FYdd6MCozWZKdmaO00mNuMaCQNgwifIIBvwKStqkc9WTys%2F0PXrBO48pSfm90AcEbBzjGiJRgmIpoKoW%2BUkwvBmd%2ByoyK9%2FIQ8nHz9nN7g%2FgopA5nBLXDRvT29mo0D3nFjO6of%2Fm0aPVA0cX2OCmbYDrdb4s%2BTl%2Fb3Cx0HBquT78mhkYTCbOd6YxFHpmb1s6QDcD%2B%2Bl003yqSDYyJdzZmoq0D8wI8NQWJcycDEPI5fgCYxaS0WubAL2QPYO4u%2BOopqWqBfe1l4gf7nkEe8Pp5UAe2vjYGxV5mN9dkOOhlrKXpWa697KXIATqSoYGQwrkumPECnPQWs3FFVrwDiwWDJNvUqUikDqIFG1zFGD7xwzLQh%2BWfrKyxhi%2BfiE8rY4YlFVQ2e6M4DwrmwPtkagsRtzufygm1nJ8bF9z%2F3aiyioMBwCeQ9y3zWT42L5V%2BRtf95N0aGjjjlfzKMB04euz6YWqkhoVuaKGo2WFlt82S4B9PPxa8d8v3bTwxZgfY%2FAXJaAsbET7i9t89h9XF9aZmjR06YbPLi4F0%2FMAbq0njAWnW6oSW9xYXl7Rugx1p8DpKKtzyBr%2FpzNj9djuFAfKUnfLOuiu8rZpsPctz%2FDKwchAqjiYLWtPsTo%2FuyIhZoDBcO3KujlTP7F4VbI87xj5JFXwdsw5C1ZrP0b8N9JZZsZO0V9yhUS8n07bIc3grqIS%2Fn7wzsBGvHu5jiqpMIwz8SIrwY6sgHXwF3s5SpzUmT%2Bd%2FpaOxFaSWK69qbq1fh%2Bv92O3hmad2L%2F82iL5dKCVYcfn5cEbl1mCCJGLbj%2B1JACCSn%2Bb0IDzbqm9PlhBh09lYsoQFVr%2FMaOCh2jlc6UJX%2BWuWuVQ1DercdbawxGlkLwfrDweiqdswRdEHNP5yU6LkedJVvBuG%2Ffm4Kl05DMaCUpKB5zH0b04M16ASPcw5wiPUpNMuyudj1Ko4wwx2sOZt%2BDxbVJHEdP&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240301T194332Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYY7GY5ACN%2F20240301%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=746656b69aa01f343be150d203fa4d604f6da3aad6570344a69b5eda22f1ac03&hash=da2637ff2882758ae7d5d5d82b08ed3897533c9ea8fae448ed1b18c618c8b49a&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0168192320301945&tid=spdf-f3eb7708-9f66-472e-82f4-ba59d2040f74&sid=54e759056934f749ce7937c-05135ec4bd54gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0f1557550402015c085500&rr=85dba2f5b9a06a41&cc=us)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 US-Pnp WAT  fpar\n",
      "48 US-KS3 WET  fpar\n",
      "70 DE-RuS CRO  fpar\n",
      "111 US-Snf GRA  fpar\n",
      "147 IT-La2 ENF  site_df\n",
      "155 US-WPT WET  fpar\n",
      "167 US-LWW GRA  site_df\n",
      "215 CD-Ygb MF  site_df\n",
      "218 CG-Tch SAV  site_df\n",
      "248 DE-Akm WET  fpar\n",
      "270 US-Me4 ENF  site_df\n",
      "279 US-ORv WET  fpar\n",
      "282 FR-Tou GRA  fpar\n",
      "297 GH-Ank EBF  site_df\n",
      "316 US-HB1 WET  fpar\n",
      "320 US-UM3 WAT  fpar\n",
      "370 IT-Ro1 DBF  fpar\n"
     ]
    }
   ],
   "source": [
    "par_counter = 0\n",
    "combined_data = []\n",
    "for i in range(len(combined_names)):\n",
    "    name = combined_names[i]\n",
    "    type = combined_types[i]\n",
    "    ec_data = combined_ec[combined_ec[\"name\"] == name]\n",
    "    # # -------------Filter GPP------------------------\n",
    "    # # The gap filled data is less than 20%\n",
    "    ec_data = ec_data[ec_data[\"NEE_VUT_REF_QC\"] > 0.8]\n",
    "\n",
    "    # Uncertaninity in NEE is less than 3  C m−2 d−1\n",
    "    ec_data = ec_data[ec_data[\"NEE_VUT_REF_RANDUNC\"] < 3]\n",
    "\n",
    "    # Difference between GPP day and night is less than 3 C m−2 d−1\n",
    "    # idx_good_gpp = abs(ec_data[\"GPP_DT_VUT_REF\"] - ec_data[\"GPP_NT_VUT_REF\"]) < 3\n",
    "    # gpp_day = ec_data[\"GPP_DT_VUT_REF\"][idx_good_gpp]\n",
    "    # gpp_night = ec_data[\"GPP_NT_VUT_REF\"][idx_good_gpp]\n",
    "    gpp_night = ec_data[\"GPP_NT_VUT_REF\"]\n",
    "    gpp_day = ec_data[\"GPP_DT_VUT_REF\"]\n",
    "\n",
    "    # # Take the mean of the day and night GPP\n",
    "    gpp = (gpp_day + gpp_night) / 2\n",
    "\n",
    "    # Make sure there is no negative GPP\n",
    "    gpp = gpp[gpp > 0]\n",
    "\n",
    "    # ----------------Filter PAR-----------------------\n",
    "    par_qc = ec_data.loc[gpp.index, \"PPFD_IN_QC\"]\n",
    "    sw_qc = ec_data.loc[gpp.index, \"SW_IN_F_QC\"]\n",
    "\n",
    "    idx_good_par = par_qc[par_qc > 0.8]\n",
    "    idx_good_sw = sw_qc[sw_qc > 0.8]\n",
    "\n",
    "    par = ec_data.loc[idx_good_par.index, \"PPFD_IN\"]\n",
    "    sw = ec_data.loc[idx_good_sw.index, \"SW_IN_F\"]\n",
    "\n",
    "    # # Alternative way to calculate PAR\n",
    "    if par.empty:\n",
    "        # convert SW from w m-2 to umol m-2 s-1 then multiply by 0.45 to get PAR\n",
    "        par = sw * 4.57 * 0.45\n",
    "        par_counter += 1\n",
    "\n",
    "    gpp = gpp[par.index]\n",
    "    gpp = gpp.to_frame(\"gpp\")\n",
    "    par = par.to_frame(\"par\")\n",
    "\n",
    "    if par.empty or gpp.empty:\n",
    "        print(i, name, type, \"Par or GPP is empty\")\n",
    "        continue\n",
    "\n",
    "    site_ec = pd.concat([gpp, par], axis=1)\n",
    "\n",
    "    # ----------------Filter Reflectance and FPAR-----------------------\n",
    "    site_refl = combined_refl[combined_refl[\"name\"] == name]\n",
    "\n",
    "    site_refl.index = pd.to_datetime(site_refl.index, format=\"%Y-%m-%d\")\n",
    "    site_refl = site_refl[site_refl.index.isin(site_ec.index)]\n",
    "    filtered_refl = site_refl[\n",
    "        (\n",
    "            site_refl[\"MCD43A4_061_BRDF_Albedo_Band_Mandatory_Quality_Band1_MODLAND\"]\n",
    "            == \"0b000\"\n",
    "        )\n",
    "        & (\n",
    "            site_refl[\"MCD43A4_061_BRDF_Albedo_Band_Mandatory_Quality_Band2_MODLAND\"]\n",
    "            == \"0b000\"\n",
    "        )\n",
    "    ].copy()\n",
    "\n",
    "    site_red = filtered_refl[[\"MCD43A4_061_Nadir_Reflectance_Band1\"]].rename(\n",
    "        columns={\"MCD43A4_061_Nadir_Reflectance_Band1\": \"red\"}\n",
    "    )\n",
    "    site_nir = filtered_refl[[\"MCD43A4_061_Nadir_Reflectance_Band2\"]].rename(\n",
    "        columns={\"MCD43A4_061_Nadir_Reflectance_Band2\": \"nir\"}\n",
    "    )\n",
    "\n",
    "    site_fpar = combined_fpar[combined_fpar[\"name\"] == name]\n",
    "    site_fpar.index = pd.to_datetime(site_fpar.index, format=\"%Y-%m-%d\")\n",
    "\n",
    "    filtered_fpar = site_fpar[\n",
    "        (site_fpar[\"MCD15A3H_061_FparLai_QC_MODLAND\"] == \"0b0\")\n",
    "        & (site_fpar[\"MCD15A3H_061_FparLai_QC_DeadDetector\"] == \"0b0\")\n",
    "        & (site_fpar[\"MCD15A3H_061_FparLai_QC_CloudState\"] == \"0b00\")\n",
    "        & (site_fpar[\"MCD15A3H_061_FparLai_QC_SCF_QC\"].isin([\"0b000\", \"0b001\"]))\n",
    "    ].copy()\n",
    "    if filtered_fpar.empty:\n",
    "        print(i, name, type, \" fpar\")\n",
    "        continue\n",
    "    fpar_tmp = filtered_fpar[\"MCD15A3H_061_Fpar_500m\"]\n",
    "    site_fpar = fpar_tmp.resample(\"D\").interpolate(\"linear\")\n",
    "    site_fpar = site_fpar.to_frame(\"fpar\")\n",
    "    site_fpar = site_fpar[site_fpar.index.isin(site_ec.index)]\n",
    "\n",
    "    lai_tmp = filtered_fpar[\"MCD15A3H_061_Lai_500m\"]\n",
    "    site_lai = lai_tmp.resample(\"D\").interpolate(\"linear\")\n",
    "    site_lai = site_lai.to_frame(\"lai\")\n",
    "    site_lai = site_lai[site_lai.index.isin(site_ec.index)]\n",
    "    # Merge the dataframes\n",
    "    site_df = (\n",
    "        site_ec.merge(site_red, left_index=True, right_index=True)\n",
    "        .merge(site_nir, left_index=True, right_index=True)\n",
    "        .merge(site_fpar, left_index=True, right_index=True)\n",
    "        .merge(site_lai, left_index=True, right_index=True)\n",
    "    )\n",
    "    if site_df.empty:\n",
    "        print(i, name, type, \" site_df\")\n",
    "        continue\n",
    "    # Calculate the NDVI, NIRv, NIRvp, Fesc, and LUE\n",
    "    site_df.loc[:, \"ndvi\"] = (site_df[\"nir\"] - site_df[\"red\"]) / (\n",
    "        site_df[\"nir\"] + site_df[\"red\"]\n",
    "    )\n",
    "    site_df.loc[:, \"nirv\"] = site_df[\"ndvi\"] * site_df[\"nir\"]\n",
    "    site_df.loc[:, \"nirvp\"] = site_df[\"nirv\"] * site_df[\"par\"]\n",
    "    site_df.loc[:, \"fesc\"] = site_df[\"nirv\"] / site_df[\"fpar\"]\n",
    "    site_df.loc[:, \"fesc_p\"] = site_df[\"fesc\"] * site_df[\"par\"]\n",
    "    site_df.loc[:, \"apar\"] = site_df[\"fpar\"] * site_df[\"par\"]\n",
    "    site_df.loc[:, \"fesc_n\"] = site_df[\"nirv\"] / site_df[\"apar\"]\n",
    "\n",
    "    site_df.loc[:, \"lue\"] = site_df[\"gpp\"] / (site_df[\"par\"] * site_df[\"fpar\"])\n",
    "    site_df = site_df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    cleaned_site_df = remove_outliers(site_df, 3).copy()\n",
    "    cleaned_site_df.loc[:, \"name\"] = name\n",
    "    cleaned_site_df.loc[:, \"type\"] = type\n",
    "    combined_data.append(cleaned_site_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('number of sites with par being approximated:', 64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"number of sites with par being approximated:\", par_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(combined_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the GLASS LAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "glass_dir = \"/home/hamid/mnt/nas/GLASS/csv_files/\"\n",
    "names = df[\"name\"].unique()\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "for name in names:\n",
    "    glass_lai = pd.read_csv(glass_dir + name + \".csv\")\n",
    "    if glass_lai.empty:\n",
    "        print(name, \": No Glass LAI\")\n",
    "        continue\n",
    "    glass_lai.set_index(\"Unnamed: 0\", inplace=True)\n",
    "    glass_lai.index = pd.to_datetime(glass_lai.index, format=\"%Y-%m-%d\")\n",
    "    glass_lai_daily = glass_lai.resample(\"D\").interpolate(\"linear\")\n",
    "    glass_lai_daily.rename(columns={\"LAI\": \"glass_lai\"}, inplace=True)\n",
    "    glass_lai_daily.index.name = \"\"\n",
    "    tmp_df = df[df[\"name\"] == name]\n",
    "    tmp_df = tmp_df.merge(glass_lai_daily, left_index=True, right_index=True)\n",
    "\n",
    "    # Append the tmp_df to the result_df\n",
    "    result_df = pd.concat([result_df, tmp_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"../outputs/data_clean_glass_lai_icos2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpp</th>\n",
       "      <th>par</th>\n",
       "      <th>red</th>\n",
       "      <th>nir</th>\n",
       "      <th>fpar</th>\n",
       "      <th>lai</th>\n",
       "      <th>ndvi</th>\n",
       "      <th>nirv</th>\n",
       "      <th>nirvp</th>\n",
       "      <th>fesc</th>\n",
       "      <th>fesc_p</th>\n",
       "      <th>apar</th>\n",
       "      <th>fesc_n</th>\n",
       "      <th>lue</th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2002-07-04</th>\n",
       "      <td>0.019643</td>\n",
       "      <td>381.342964</td>\n",
       "      <td>0.2345</td>\n",
       "      <td>0.3295</td>\n",
       "      <td>0.1300</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.168440</td>\n",
       "      <td>0.055501</td>\n",
       "      <td>21.164873</td>\n",
       "      <td>0.426930</td>\n",
       "      <td>162.806712</td>\n",
       "      <td>49.574585</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>US-Cop</td>\n",
       "      <td>GRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-07-16</th>\n",
       "      <td>0.030749</td>\n",
       "      <td>601.182815</td>\n",
       "      <td>0.2305</td>\n",
       "      <td>0.3253</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.170565</td>\n",
       "      <td>0.055485</td>\n",
       "      <td>33.356495</td>\n",
       "      <td>0.504407</td>\n",
       "      <td>303.240867</td>\n",
       "      <td>66.130110</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>US-Cop</td>\n",
       "      <td>GRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-07-18</th>\n",
       "      <td>0.000467</td>\n",
       "      <td>534.330112</td>\n",
       "      <td>0.2243</td>\n",
       "      <td>0.3158</td>\n",
       "      <td>0.1150</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.169413</td>\n",
       "      <td>0.053501</td>\n",
       "      <td>28.587007</td>\n",
       "      <td>0.465223</td>\n",
       "      <td>248.582672</td>\n",
       "      <td>61.447963</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>US-Cop</td>\n",
       "      <td>GRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-07-19</th>\n",
       "      <td>0.022495</td>\n",
       "      <td>301.396527</td>\n",
       "      <td>0.2235</td>\n",
       "      <td>0.3147</td>\n",
       "      <td>0.1175</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.169454</td>\n",
       "      <td>0.053327</td>\n",
       "      <td>16.072600</td>\n",
       "      <td>0.453848</td>\n",
       "      <td>136.788084</td>\n",
       "      <td>35.414092</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>US-Cop</td>\n",
       "      <td>GRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-07-22</th>\n",
       "      <td>0.034960</td>\n",
       "      <td>495.530127</td>\n",
       "      <td>0.2229</td>\n",
       "      <td>0.3142</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.169987</td>\n",
       "      <td>0.053410</td>\n",
       "      <td>26.466217</td>\n",
       "      <td>0.427279</td>\n",
       "      <td>211.729736</td>\n",
       "      <td>61.941266</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>US-Cop</td>\n",
       "      <td>GRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-21</th>\n",
       "      <td>3.927745</td>\n",
       "      <td>254.209917</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>0.2466</td>\n",
       "      <td>0.8350</td>\n",
       "      <td>3.150</td>\n",
       "      <td>0.701863</td>\n",
       "      <td>0.173080</td>\n",
       "      <td>43.998526</td>\n",
       "      <td>0.207281</td>\n",
       "      <td>52.692846</td>\n",
       "      <td>212.265280</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>0.018504</td>\n",
       "      <td>US-KS2</td>\n",
       "      <td>CSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-22</th>\n",
       "      <td>3.360560</td>\n",
       "      <td>194.859792</td>\n",
       "      <td>0.0431</td>\n",
       "      <td>0.2466</td>\n",
       "      <td>0.8275</td>\n",
       "      <td>3.025</td>\n",
       "      <td>0.702451</td>\n",
       "      <td>0.173224</td>\n",
       "      <td>33.754465</td>\n",
       "      <td>0.209335</td>\n",
       "      <td>40.790894</td>\n",
       "      <td>161.246478</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.020841</td>\n",
       "      <td>US-KS2</td>\n",
       "      <td>CSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-23</th>\n",
       "      <td>4.393695</td>\n",
       "      <td>249.624937</td>\n",
       "      <td>0.0431</td>\n",
       "      <td>0.2466</td>\n",
       "      <td>0.8200</td>\n",
       "      <td>2.900</td>\n",
       "      <td>0.702451</td>\n",
       "      <td>0.173224</td>\n",
       "      <td>43.241123</td>\n",
       "      <td>0.211249</td>\n",
       "      <td>52.733076</td>\n",
       "      <td>204.692449</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>0.021465</td>\n",
       "      <td>US-KS2</td>\n",
       "      <td>CSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-24</th>\n",
       "      <td>2.999990</td>\n",
       "      <td>180.187104</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>0.2469</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>2.775</td>\n",
       "      <td>0.702172</td>\n",
       "      <td>0.173366</td>\n",
       "      <td>31.238351</td>\n",
       "      <td>0.213374</td>\n",
       "      <td>38.447201</td>\n",
       "      <td>146.402022</td>\n",
       "      <td>0.001184</td>\n",
       "      <td>0.020491</td>\n",
       "      <td>US-KS2</td>\n",
       "      <td>CSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-12-25</th>\n",
       "      <td>2.108105</td>\n",
       "      <td>100.397854</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>0.2471</td>\n",
       "      <td>0.8050</td>\n",
       "      <td>2.650</td>\n",
       "      <td>0.702377</td>\n",
       "      <td>0.173557</td>\n",
       "      <td>17.424783</td>\n",
       "      <td>0.215599</td>\n",
       "      <td>21.645693</td>\n",
       "      <td>80.820273</td>\n",
       "      <td>0.002147</td>\n",
       "      <td>0.026084</td>\n",
       "      <td>US-KS2</td>\n",
       "      <td>CSH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>318168 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 gpp         par     red     nir    fpar    lai      ndvi  \\\n",
       "2002-07-04  0.019643  381.342964  0.2345  0.3295  0.1300  0.200  0.168440   \n",
       "2002-07-16  0.030749  601.182815  0.2305  0.3253  0.1100  0.200  0.170565   \n",
       "2002-07-18  0.000467  534.330112  0.2243  0.3158  0.1150  0.200  0.169413   \n",
       "2002-07-19  0.022495  301.396527  0.2235  0.3147  0.1175  0.200  0.169454   \n",
       "2002-07-22  0.034960  495.530127  0.2229  0.3142  0.1250  0.200  0.169987   \n",
       "...              ...         ...     ...     ...     ...    ...       ...   \n",
       "2006-12-21  3.927745  254.209917  0.0432  0.2466  0.8350  3.150  0.701863   \n",
       "2006-12-22  3.360560  194.859792  0.0431  0.2466  0.8275  3.025  0.702451   \n",
       "2006-12-23  4.393695  249.624937  0.0431  0.2466  0.8200  2.900  0.702451   \n",
       "2006-12-24  2.999990  180.187104  0.0432  0.2469  0.8125  2.775  0.702172   \n",
       "2006-12-25  2.108105  100.397854  0.0432  0.2471  0.8050  2.650  0.702377   \n",
       "\n",
       "                nirv      nirvp      fesc      fesc_p        apar    fesc_n  \\\n",
       "2002-07-04  0.055501  21.164873  0.426930  162.806712   49.574585  0.001120   \n",
       "2002-07-16  0.055485  33.356495  0.504407  303.240867   66.130110  0.000839   \n",
       "2002-07-18  0.053501  28.587007  0.465223  248.582672   61.447963  0.000871   \n",
       "2002-07-19  0.053327  16.072600  0.453848  136.788084   35.414092  0.001506   \n",
       "2002-07-22  0.053410  26.466217  0.427279  211.729736   61.941266  0.000862   \n",
       "...              ...        ...       ...         ...         ...       ...   \n",
       "2006-12-21  0.173080  43.998526  0.207281   52.692846  212.265280  0.000815   \n",
       "2006-12-22  0.173224  33.754465  0.209335   40.790894  161.246478  0.001074   \n",
       "2006-12-23  0.173224  43.241123  0.211249   52.733076  204.692449  0.000846   \n",
       "2006-12-24  0.173366  31.238351  0.213374   38.447201  146.402022  0.001184   \n",
       "2006-12-25  0.173557  17.424783  0.215599   21.645693   80.820273  0.002147   \n",
       "\n",
       "                 lue    name type  \n",
       "2002-07-04  0.000396  US-Cop  GRA  \n",
       "2002-07-16  0.000465  US-Cop  GRA  \n",
       "2002-07-18  0.000008  US-Cop  GRA  \n",
       "2002-07-19  0.000635  US-Cop  GRA  \n",
       "2002-07-22  0.000564  US-Cop  GRA  \n",
       "...              ...     ...  ...  \n",
       "2006-12-21  0.018504  US-KS2  CSH  \n",
       "2006-12-22  0.020841  US-KS2  CSH  \n",
       "2006-12-23  0.021465  US-KS2  CSH  \n",
       "2006-12-24  0.020491  US-KS2  CSH  \n",
       "2006-12-25  0.026084  US-KS2  CSH  \n",
       "\n",
       "[318168 rows x 16 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding clumping index values (if asked by reviewer later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ci = xr.open_dataset(\n",
    "#     \"../data/EC/Global_Clumping_Index_1531/data/global_clumping_index_geographic.nc\"\n",
    "# )\n",
    "# ci = ci.__xarray_dataarray_variable__\n",
    "# ameriflux_coords = pd.read_csv(\"../data/Ameriflux_coords.csv\")\n",
    "# fluxnet_coords = pd.read_csv(\"../data/Fluxnet_coords.csv\")\n",
    "# merged_coords = pd.concat([ameriflux_coords, fluxnet_coords], ignore_index=True)\n",
    "# merged_coords.drop_duplicates(subset=merged_coords.columns[0], inplace=True)\n",
    "# merged_coords.reset_index(drop=True, inplace=True)\n",
    "# merged_coords.rename({\"Name\": \"name\"}, axis=1, inplace=True)\n",
    "# pd_all = pd.merge(df, merged_coords[[\"name\", \"Lat\", \"Lon\"]], on=\"name\", how=\"left\")\n",
    "# pd_all.set_index(df.index, inplace=True)\n",
    "# names = pd_all[\"name\"].unique()\n",
    "# for name in names:\n",
    "#     lat = pd_all[pd_all[\"name\"] == name][\"Lat\"].values[0]\n",
    "#     lon = pd_all[pd_all[\"name\"] == name][\"Lon\"].values[0]\n",
    "#     ci_point = ci.sel(x=lon, y=lat, method=\"nearest\").values\n",
    "#     pd_all.loc[pd_all[\"name\"] == name, \"ci\"] = ci_point\n",
    "# pd_all.to_csv(\"../outputs/data_clean_fpar_lai_ci2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dscovr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
