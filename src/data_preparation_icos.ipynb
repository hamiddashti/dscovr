{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from scipy.stats import zscore, linregress\n",
    "from pandas.plotting import scatter_matrix\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "def remove_outliers(df, threshold=3):\n",
    "    # Initialize a boolean mask to keep track of rows to drop\n",
    "    outlier_rows_mask = np.zeros(len(df), dtype=bool)\n",
    "\n",
    "    # Iterate over each column\n",
    "    for col in df.columns:\n",
    "        # Skip the \"t1\" and \"t2\" columns\n",
    "        if col == \"t1\" or col == \"t2\":\n",
    "            continue\n",
    "\n",
    "        # Calculate the mean and standard deviation of the column\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "\n",
    "        # Find outliers in this column\n",
    "        outliers = (df[col] - mean).abs() > threshold * std\n",
    "\n",
    "        # Mark rows with outliers in this column\n",
    "        outlier_rows_mask = np.logical_or(outlier_rows_mask, outliers)\n",
    "\n",
    "    # Drop rows with outliers\n",
    "    cleaned_df = df[~outlier_rows_mask]\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data and make them consistent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluxnet_info = pd.read_csv(\"../data/EC/fluxnet/sites_info.csv\")\n",
    "ameriflux_info = pd.read_csv(\"../data/EC/Ameriflux/sites_info.tsv\", delimiter=\"\\t\")\n",
    "Icos_info = pd.read_csv(\"../data/EC/ICOS/sites_info.csv\")\n",
    "\n",
    "fluxnet_names = fluxnet_info[\"ID\"].to_list()\n",
    "fluxnet_types = fluxnet_info[\"type\"].to_list()\n",
    "\n",
    "ameriflux_names = ameriflux_info[\"Site ID\"].to_list()\n",
    "ameriflux_types = ameriflux_info[\"Vegetation Abbreviation (IGBP)\"].to_list()\n",
    "\n",
    "icos_names = Icos_info[\"ID\"].to_list()\n",
    "icos_types = Icos_info[\"type\"].to_list()\n",
    "icos_id = Icos_info[\"id_number\"].to_list()\n",
    "icos_dict = dict(zip(icos_id, icos_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_names = list(set(ameriflux_names + fluxnet_names + icos_names))\n",
    "combined_types = []\n",
    "for name in combined_names:\n",
    "    if name in ameriflux_names and name in fluxnet_names:\n",
    "        # Choose a type from either fluxnet_types or ameriflux_types\n",
    "        combined_types.append(fluxnet_types[fluxnet_names.index(name)])\n",
    "    elif name in ameriflux_names:\n",
    "        combined_types.append(ameriflux_types[ameriflux_names.index(name)])\n",
    "    elif name in icos_names:\n",
    "        combined_types.append(icos_types[icos_names.index(name)])\n",
    "    else:\n",
    "        combined_types.append(fluxnet_types[fluxnet_names.index(name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ec = []\n",
    "\n",
    "for i in range(len(combined_names)):\n",
    "    site_name = combined_names[i]\n",
    "    site_type = combined_types[i]\n",
    "\n",
    "    if site_name in ameriflux_names:\n",
    "        file = glob.glob(\"../data/EC/Ameriflux/AMF_\" + site_name + \"*DD*\")\n",
    "    elif site_name in icos_names:\n",
    "        file = glob.glob(\n",
    "            \"../data/EC/ICOS/Data/FLX_\" + site_name + \"*\" + \"/*FULLSET_DD*\"\n",
    "        )\n",
    "    else:\n",
    "        file = glob.glob(\"../data/EC/fluxnet/FLX_\" + site_name + \"*DD*\")\n",
    "\n",
    "    ec = pd.read_csv(file[0])\n",
    "    ec.loc[:, \"type\"] = site_type\n",
    "    ec.loc[:, \"name\"] = site_name\n",
    "    ec.index = pd.to_datetime(ec[\"TIMESTAMP\"], format=\"%Y%m%d\")\n",
    "    combined_ec.append(ec)\n",
    "\n",
    "combined_ec = pd.concat(combined_ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for five day fpar\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "MCD43_fluxnet = []\n",
    "MCD43_ameriflux = []\n",
    "MCD43_icos = []\n",
    "\n",
    "MCD15_fluxnet = []\n",
    "MCD15_ameriflux = []\n",
    "MCD15_icos = []\n",
    "\n",
    "# Loop over batches (#5) of downloaded data\n",
    "for i in range(1, 6):\n",
    "    refl_fluxnet = glob.glob(\n",
    "        \"../data/EC/fluxnet/sat_data/*batch\" + str(i) + \"*MCD43A4-061-results.csv\"\n",
    "    )\n",
    "    sat_refl_fluxnet = pd.read_csv(refl_fluxnet[0])\n",
    "    sat_refl_fluxnet.loc[:, \"time\"] = pd.to_datetime(sat_refl_fluxnet[\"Date\"])\n",
    "    sat_refl_fluxnet.set_index(sat_refl_fluxnet[\"Date\"], inplace=True)\n",
    "    MCD43_fluxnet.append(sat_refl_fluxnet)\n",
    "\n",
    "    fpar_fluxnet = glob.glob(\n",
    "        \"../data/EC/fluxnet/sat_data/*batch\" + str(i) + \"*MCD15A3H-061-results.csv\"\n",
    "    )\n",
    "    sat_fpar_fluxnet = pd.read_csv(fpar_fluxnet[0])\n",
    "    sat_fpar_fluxnet.loc[:, \"time\"] = pd.to_datetime(sat_fpar_fluxnet[\"Date\"])\n",
    "    sat_fpar_fluxnet.set_index(sat_fpar_fluxnet[\"Date\"], inplace=True)\n",
    "    MCD15_fluxnet.append(sat_fpar_fluxnet)\n",
    "\n",
    "    # Note all the ameriflux sites are in 4 batches\n",
    "    if i < 5:\n",
    "        refl_ameriflux = glob.glob(\n",
    "            \"../data/EC/Ameriflux/sat_data/*batch\" + str(i) + \"*MCD43A4-061-results.csv\"\n",
    "        )\n",
    "\n",
    "        sat_refl_ameriflux = pd.read_csv(refl_ameriflux[0])\n",
    "        sat_refl_ameriflux.loc[:, \"time\"] = pd.to_datetime(sat_refl_ameriflux[\"Date\"])\n",
    "        sat_refl_ameriflux.set_index(sat_refl_ameriflux[\"Date\"], inplace=True)\n",
    "        MCD43_ameriflux.append(sat_refl_ameriflux)\n",
    "\n",
    "        fpar_ameriflux = glob.glob(\n",
    "            \"../data/EC/Ameriflux/sat_data/*batch\"\n",
    "            + str(i)\n",
    "            + \"*MCD15A3H-061-results.csv\"\n",
    "        )\n",
    "        sat_fpar_ameriflux = pd.read_csv(fpar_ameriflux[0])  # Changed variable name\n",
    "\n",
    "        sat_fpar_ameriflux.loc[:, \"time\"] = pd.to_datetime(sat_fpar_ameriflux[\"Date\"])\n",
    "        sat_fpar_ameriflux.set_index(sat_fpar_ameriflux[\"Date\"], inplace=True)\n",
    "        MCD15_ameriflux.append(sat_fpar_ameriflux)\n",
    "    if i < 3:\n",
    "        refl_icos = glob.glob(\n",
    "            \"../data/EC/ICOS/sat_data/*batch\" + str(i) + \"*MCD43A4-061-results.csv\"\n",
    "        )\n",
    "        sat_refl_icos = pd.read_csv(refl_icos[0])\n",
    "        refl_icos = glob.glob(\n",
    "            \"../data/EC/ICOS/sat_data/*batch\" + str(i) + \"*MCD43A4-061-results.csv\"\n",
    "        )\n",
    "        sat_refl_icos = pd.read_csv(refl_icos[0])\n",
    "        sat_refl_icos[\"ID\"] = sat_refl_icos[\"ID\"].map(icos_dict)\n",
    "        sat_refl_icos.dropna(subset=[\"ID\"], inplace=True)\n",
    "        sat_refl_icos.loc[:, \"time\"] = pd.to_datetime(sat_refl_icos[\"Date\"])\n",
    "        sat_refl_icos.set_index(sat_refl_icos[\"Date\"], inplace=True)\n",
    "        MCD43_icos.append(sat_refl_icos)\n",
    "\n",
    "        fpar_icos = glob.glob(\n",
    "            \"../data/EC/ICOS/sat_data/*batch\" + str(i) + \"*MCD15A3H-061-results.csv\"\n",
    "        )\n",
    "        sat_fpar_icos = pd.read_csv(fpar_icos[0])\n",
    "        sat_fpar_icos[\"ID\"] = sat_fpar_icos[\"ID\"].map(icos_dict)\n",
    "        sat_fpar_icos.dropna(subset=[\"ID\"], inplace=True)\n",
    "\n",
    "        sat_fpar_icos.loc[:, \"time\"] = pd.to_datetime(sat_fpar_icos[\"Date\"])\n",
    "        sat_fpar_icos.set_index(sat_fpar_icos[\"Date\"], inplace=True)\n",
    "        MCD15_icos.append(sat_fpar_icos)\n",
    "\n",
    "\n",
    "refl_fluxnet = pd.concat(MCD43_fluxnet)\n",
    "refl_fluxnet = refl_fluxnet.rename(columns={\"ID\": \"name\"})\n",
    "fpar_fluxnet = pd.concat(MCD15_fluxnet)\n",
    "fpar_fluxnet = fpar_fluxnet.rename(columns={\"ID\": \"name\"})\n",
    "\n",
    "refl_ameriflux = pd.concat(MCD43_ameriflux)\n",
    "refl_ameriflux = refl_ameriflux.rename(columns={\"ID\": \"name\"})\n",
    "fpar_ameriflux = pd.concat(MCD15_ameriflux)\n",
    "fpar_ameriflux = fpar_ameriflux.rename(columns={\"ID\": \"name\"})\n",
    "\n",
    "refl_icos = pd.concat(MCD43_icos)\n",
    "refl_icos = refl_icos.rename(columns={\"ID\": \"name\"})\n",
    "fpar_icos = pd.concat(MCD15_icos)\n",
    "fpar_icos = fpar_icos.rename(columns={\"ID\": \"name\"})\n",
    "\n",
    "\n",
    "combined_refl = []\n",
    "combined_fpar = []\n",
    "\n",
    "for name in combined_names:\n",
    "    if name in ameriflux_names:\n",
    "        selected_refl = refl_ameriflux[refl_ameriflux[\"name\"] == name]\n",
    "        selected_fpar = fpar_ameriflux[fpar_ameriflux[\"name\"] == name]\n",
    "    elif name in icos_names:\n",
    "        selected_refl = refl_icos[refl_icos[\"name\"] == name]\n",
    "        selected_fpar = fpar_icos[fpar_icos[\"name\"] == name]\n",
    "    else:\n",
    "        selected_refl = refl_fluxnet[refl_fluxnet[\"name\"] == name]\n",
    "        selected_fpar = fpar_fluxnet[fpar_fluxnet[\"name\"] == name]\n",
    "\n",
    "    combined_refl.append(selected_refl)\n",
    "    combined_fpar.append(selected_fpar)\n",
    "\n",
    "combined_refl = pd.concat(combined_refl)\n",
    "combined_fpar = pd.concat(combined_fpar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flags come from [This paper](https://pdf.sciencedirectassets.com/271723/1-s2.0-S0168192320X00062/1-s2.0-S0168192320301945/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDsaCXVzLWVhc3QtMSJGMEQCIH4OB25%2B%2BrWEsBRZZ4CMPUgYETlwWwu0kcC2JF5q5VHKAiAysHZntlDvw8izOPtHcQa1YDDXXdQ%2ByZrXPCaxn2NdLiqzBQg0EAUaDDA1OTAwMzU0Njg2NSIMhywMz6eScMTpU2DpKpAFhCNp4JUVE3sxgm2Ly4FDKc%2BSr7igdIpyk9TsBrE%2F0lC51AZ4N3waj2MpXB%2BA2dorxigw%2BZqxNIgin%2BQFdFvFUN0k6mgrcLTBkS9FhRXOMtTKHvqFgR6FAHKZynNnFuxkIM6eV3dZLRHS0R2yeyRpHxGUk%2FYdd6MCozWZKdmaO00mNuMaCQNgwifIIBvwKStqkc9WTys%2F0PXrBO48pSfm90AcEbBzjGiJRgmIpoKoW%2BUkwvBmd%2ByoyK9%2FIQ8nHz9nN7g%2FgopA5nBLXDRvT29mo0D3nFjO6of%2Fm0aPVA0cX2OCmbYDrdb4s%2BTl%2Fb3Cx0HBquT78mhkYTCbOd6YxFHpmb1s6QDcD%2B%2Bl003yqSDYyJdzZmoq0D8wI8NQWJcycDEPI5fgCYxaS0WubAL2QPYO4u%2BOopqWqBfe1l4gf7nkEe8Pp5UAe2vjYGxV5mN9dkOOhlrKXpWa697KXIATqSoYGQwrkumPECnPQWs3FFVrwDiwWDJNvUqUikDqIFG1zFGD7xwzLQh%2BWfrKyxhi%2BfiE8rY4YlFVQ2e6M4DwrmwPtkagsRtzufygm1nJ8bF9z%2F3aiyioMBwCeQ9y3zWT42L5V%2BRtf95N0aGjjjlfzKMB04euz6YWqkhoVuaKGo2WFlt82S4B9PPxa8d8v3bTwxZgfY%2FAXJaAsbET7i9t89h9XF9aZmjR06YbPLi4F0%2FMAbq0njAWnW6oSW9xYXl7Rugx1p8DpKKtzyBr%2FpzNj9djuFAfKUnfLOuiu8rZpsPctz%2FDKwchAqjiYLWtPsTo%2FuyIhZoDBcO3KujlTP7F4VbI87xj5JFXwdsw5C1ZrP0b8N9JZZsZO0V9yhUS8n07bIc3grqIS%2Fn7wzsBGvHu5jiqpMIwz8SIrwY6sgHXwF3s5SpzUmT%2Bd%2FpaOxFaSWK69qbq1fh%2Bv92O3hmad2L%2F82iL5dKCVYcfn5cEbl1mCCJGLbj%2B1JACCSn%2Bb0IDzbqm9PlhBh09lYsoQFVr%2FMaOCh2jlc6UJX%2BWuWuVQ1DercdbawxGlkLwfrDweiqdswRdEHNP5yU6LkedJVvBuG%2Ffm4Kl05DMaCUpKB5zH0b04M16ASPcw5wiPUpNMuyudj1Ko4wwx2sOZt%2BDxbVJHEdP&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240301T194332Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYY7GY5ACN%2F20240301%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=746656b69aa01f343be150d203fa4d604f6da3aad6570344a69b5eda22f1ac03&hash=da2637ff2882758ae7d5d5d82b08ed3897533c9ea8fae448ed1b18c618c8b49a&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0168192320301945&tid=spdf-f3eb7708-9f66-472e-82f4-ba59d2040f74&sid=54e759056934f749ce7937c-05135ec4bd54gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0f1557550402015c085500&rr=85dba2f5b9a06a41&cc=us)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 US-ORv WET  fpar\n",
      "3 US-Pnp WAT  fpar\n",
      "4 US-LWW GRA  site_df\n",
      "53 CD-Ygb MF  site_df\n",
      "65 IT-La2 ENF  site_df\n",
      "66 CG-Tch SAV  site_df\n",
      "136 IT-Ro1 DBF  fpar\n",
      "147 US-Snf GRA  fpar\n",
      "153 DE-RuS CRO  fpar\n",
      "178 US-UM3 WAT  fpar\n",
      "197 GH-Ank EBF  site_df\n",
      "213 US-HB1 WET  fpar\n",
      "217 US-Me4 ENF  site_df\n",
      "236 US-KS3 WET  fpar\n",
      "258 DE-Akm WET  fpar\n",
      "272 FR-Tou GRA  fpar\n",
      "342 US-WPT WET  fpar\n"
     ]
    }
   ],
   "source": [
    "par_counter = 0\n",
    "combined_data = []\n",
    "for i in range(len(combined_names)):\n",
    "    name = combined_names[i]\n",
    "    type = combined_types[i]\n",
    "    ec_data = combined_ec[combined_ec[\"name\"] == name]\n",
    "    # # -------------Filter GPP------------------------\n",
    "    # # The gap filled data is less than 20%\n",
    "    ec_data = ec_data[ec_data[\"NEE_VUT_REF_QC\"] > 0.8]\n",
    "\n",
    "    # Uncertaninity in NEE is less than 3  C m−2 d−1\n",
    "    ec_data = ec_data[ec_data[\"NEE_VUT_REF_RANDUNC\"] < 3]\n",
    "\n",
    "    # Difference between GPP day and night is less than 3 C m−2 d−1\n",
    "    # idx_good_gpp = abs(ec_data[\"GPP_DT_VUT_REF\"] - ec_data[\"GPP_NT_VUT_REF\"]) < 3\n",
    "    # gpp_day = ec_data[\"GPP_DT_VUT_REF\"][idx_good_gpp]\n",
    "    # gpp_night = ec_data[\"GPP_NT_VUT_REF\"][idx_good_gpp]\n",
    "    gpp_night = ec_data[\"GPP_NT_VUT_REF\"]\n",
    "    gpp_day = ec_data[\"GPP_DT_VUT_REF\"]\n",
    "\n",
    "    # # Take the mean of the day and night GPP\n",
    "    gpp = (gpp_day + gpp_night) / 2\n",
    "\n",
    "    # Make sure there is no negative GPP\n",
    "    gpp = gpp[gpp > 0]\n",
    "\n",
    "    # ----------------Filter PAR-----------------------\n",
    "    par_qc = ec_data.loc[gpp.index, \"PPFD_IN_QC\"]\n",
    "    sw_qc = ec_data.loc[gpp.index, \"SW_IN_F_QC\"]\n",
    "\n",
    "    idx_good_par = par_qc[par_qc > 0.8]\n",
    "    idx_good_sw = sw_qc[sw_qc > 0.8]\n",
    "\n",
    "    par = ec_data.loc[idx_good_par.index, \"PPFD_IN\"]\n",
    "    sw = ec_data.loc[idx_good_sw.index, \"SW_IN_F\"]\n",
    "\n",
    "    # # Alternative way to calculate PAR\n",
    "    if par.empty:\n",
    "        # convert SW from w m-2 to umol m-2 s-1 then multiply by 0.45 to get PAR\n",
    "        par = sw * 4.57 * 0.45\n",
    "        par_counter += 1\n",
    "\n",
    "    gpp = gpp[par.index]\n",
    "    gpp = gpp.to_frame(\"gpp\")\n",
    "    par = par.to_frame(\"par\")\n",
    "\n",
    "    if par.empty or gpp.empty:\n",
    "        print(i, name, type, \"Par or GPP is empty\")\n",
    "        continue\n",
    "\n",
    "    site_ec = pd.concat([gpp, par], axis=1)\n",
    "\n",
    "    # ----------------Filter Reflectance and FPAR-----------------------\n",
    "    site_refl = combined_refl[combined_refl[\"name\"] == name]\n",
    "\n",
    "    site_refl.index = pd.to_datetime(site_refl.index, format=\"%Y-%m-%d\")\n",
    "    site_refl = site_refl[site_refl.index.isin(site_ec.index)]\n",
    "    filtered_refl = site_refl[\n",
    "        (\n",
    "            site_refl[\"MCD43A4_061_BRDF_Albedo_Band_Mandatory_Quality_Band1_MODLAND\"]\n",
    "            == \"0b000\"\n",
    "        )\n",
    "        & (\n",
    "            site_refl[\"MCD43A4_061_BRDF_Albedo_Band_Mandatory_Quality_Band2_MODLAND\"]\n",
    "            == \"0b000\"\n",
    "        )\n",
    "    ].copy()\n",
    "\n",
    "    site_red = filtered_refl[[\"MCD43A4_061_Nadir_Reflectance_Band1\"]].rename(\n",
    "        columns={\"MCD43A4_061_Nadir_Reflectance_Band1\": \"red\"}\n",
    "    )\n",
    "    site_nir = filtered_refl[[\"MCD43A4_061_Nadir_Reflectance_Band2\"]].rename(\n",
    "        columns={\"MCD43A4_061_Nadir_Reflectance_Band2\": \"nir\"}\n",
    "    )\n",
    "\n",
    "    site_fpar = combined_fpar[combined_fpar[\"name\"] == name]\n",
    "    site_fpar.index = pd.to_datetime(site_fpar.index, format=\"%Y-%m-%d\")\n",
    "\n",
    "    filtered_fpar = site_fpar[\n",
    "        (site_fpar[\"MCD15A3H_061_FparLai_QC_MODLAND\"] == \"0b0\")\n",
    "        & (site_fpar[\"MCD15A3H_061_FparLai_QC_DeadDetector\"] == \"0b0\")\n",
    "        & (site_fpar[\"MCD15A3H_061_FparLai_QC_CloudState\"] == \"0b00\")\n",
    "        & (site_fpar[\"MCD15A3H_061_FparLai_QC_SCF_QC\"].isin([\"0b000\", \"0b001\"]))\n",
    "    ].copy()\n",
    "    if filtered_fpar.empty:\n",
    "        print(i, name, type, \" fpar\")\n",
    "        continue\n",
    "    fpar_tmp = filtered_fpar[\"MCD15A3H_061_Fpar_500m\"]\n",
    "    site_fpar = fpar_tmp.resample(\"D\").interpolate(\"linear\")\n",
    "    site_fpar = site_fpar.to_frame(\"fpar\")\n",
    "    site_fpar = site_fpar[site_fpar.index.isin(site_ec.index)]\n",
    "\n",
    "    lai_tmp = filtered_fpar[\"MCD15A3H_061_Lai_500m\"]\n",
    "    site_lai = lai_tmp.resample(\"D\").interpolate(\"linear\")\n",
    "    site_lai = site_lai.to_frame(\"lai\")\n",
    "    site_lai = site_lai[site_lai.index.isin(site_ec.index)]\n",
    "    # Merge the dataframes\n",
    "    site_df = (\n",
    "        site_ec.merge(site_red, left_index=True, right_index=True)\n",
    "        .merge(site_nir, left_index=True, right_index=True)\n",
    "        .merge(site_fpar, left_index=True, right_index=True)\n",
    "        .merge(site_lai, left_index=True, right_index=True)\n",
    "    )\n",
    "    if site_df.empty:\n",
    "        print(i, name, type, \" site_df\")\n",
    "        continue\n",
    "    # Calculate the NDVI, NIRv, NIRvp, Fesc, and LUE\n",
    "    site_df.loc[:, \"ndvi\"] = (site_df[\"nir\"] - site_df[\"red\"]) / (\n",
    "        site_df[\"nir\"] + site_df[\"red\"]\n",
    "    )\n",
    "    site_df.loc[:, \"nirv\"] = site_df[\"ndvi\"] * site_df[\"nir\"]\n",
    "    site_df.loc[:, \"nirvp\"] = site_df[\"nirv\"] * site_df[\"par\"]\n",
    "    site_df.loc[:, \"fesc\"] = site_df[\"nirv\"] / site_df[\"fpar\"]\n",
    "    site_df.loc[:, \"fesc_p\"] = site_df[\"fesc\"] * site_df[\"par\"]\n",
    "    site_df.loc[:, \"apar\"] = site_df[\"fpar\"] * site_df[\"par\"]\n",
    "    site_df.loc[:, \"fesc_n\"] = site_df[\"nirv\"] / site_df[\"apar\"]\n",
    "\n",
    "    site_df.loc[:, \"lue\"] = site_df[\"gpp\"] / (site_df[\"par\"] * site_df[\"fpar\"])\n",
    "    site_df = site_df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    cleaned_site_df = remove_outliers(site_df, 3).copy()\n",
    "    cleaned_site_df.loc[:, \"name\"] = name\n",
    "    cleaned_site_df.loc[:, \"type\"] = type\n",
    "    combined_data.append(cleaned_site_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('number of sites with par being approximated:', 64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"number of sites with par being approximated:\", par_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(combined_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the GLASS LAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "glass_dir = \"/home/hamid/mnt/nas/GLASS/csv_files/\"\n",
    "names = df[\"name\"].unique()\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "for name in names:\n",
    "    glass_lai = pd.read_csv(glass_dir + name + \".csv\")\n",
    "    if glass_lai.empty:\n",
    "        print(name, \": No Glass LAI\")\n",
    "        continue\n",
    "    glass_lai.set_index(\"Unnamed: 0\", inplace=True)\n",
    "    glass_lai.index = pd.to_datetime(glass_lai.index, format=\"%Y-%m-%d\")\n",
    "    glass_lai_daily = glass_lai.resample(\"D\").interpolate(\"linear\")\n",
    "    glass_lai_daily.rename(columns={\"LAI\": \"glass_lai\"}, inplace=True)\n",
    "    glass_lai_daily.index.name = \"\"\n",
    "    tmp_df = df[df[\"name\"] == name]\n",
    "    tmp_df = tmp_df.merge(glass_lai_daily, left_index=True, right_index=True)\n",
    "\n",
    "    # Append the tmp_df to the result_df\n",
    "    result_df = pd.concat([result_df, tmp_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"../outputs/data_clean_glass_lai_icos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpp</th>\n",
       "      <th>par</th>\n",
       "      <th>red</th>\n",
       "      <th>nir</th>\n",
       "      <th>fpar</th>\n",
       "      <th>lai</th>\n",
       "      <th>ndvi</th>\n",
       "      <th>nirv</th>\n",
       "      <th>nirvp</th>\n",
       "      <th>fesc</th>\n",
       "      <th>fesc_p</th>\n",
       "      <th>apar</th>\n",
       "      <th>fesc_n</th>\n",
       "      <th>lue</th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2002-07-04</th>\n",
       "      <td>12.562600</td>\n",
       "      <td>642.650645</td>\n",
       "      <td>0.0463</td>\n",
       "      <td>0.3376</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.758791</td>\n",
       "      <td>0.256168</td>\n",
       "      <td>164.626505</td>\n",
       "      <td>0.388133</td>\n",
       "      <td>249.434098</td>\n",
       "      <td>424.149425</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.029618</td>\n",
       "      <td>IT-PT1</td>\n",
       "      <td>DBF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-07-05</th>\n",
       "      <td>14.247250</td>\n",
       "      <td>554.901392</td>\n",
       "      <td>0.0464</td>\n",
       "      <td>0.3308</td>\n",
       "      <td>0.655000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.753977</td>\n",
       "      <td>0.249415</td>\n",
       "      <td>138.400998</td>\n",
       "      <td>0.380787</td>\n",
       "      <td>211.299234</td>\n",
       "      <td>363.460412</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>0.039199</td>\n",
       "      <td>IT-PT1</td>\n",
       "      <td>DBF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-07-06</th>\n",
       "      <td>10.387950</td>\n",
       "      <td>351.058726</td>\n",
       "      <td>0.0463</td>\n",
       "      <td>0.3307</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.754377</td>\n",
       "      <td>0.249472</td>\n",
       "      <td>87.579449</td>\n",
       "      <td>0.383804</td>\n",
       "      <td>134.737614</td>\n",
       "      <td>228.188172</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.045524</td>\n",
       "      <td>IT-PT1</td>\n",
       "      <td>DBF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-07-07</th>\n",
       "      <td>13.161150</td>\n",
       "      <td>656.717066</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.3306</td>\n",
       "      <td>0.645000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.754777</td>\n",
       "      <td>0.249529</td>\n",
       "      <td>163.870149</td>\n",
       "      <td>0.386867</td>\n",
       "      <td>254.062247</td>\n",
       "      <td>423.582507</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.031071</td>\n",
       "      <td>IT-PT1</td>\n",
       "      <td>DBF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002-07-08</th>\n",
       "      <td>13.027000</td>\n",
       "      <td>638.252843</td>\n",
       "      <td>0.0464</td>\n",
       "      <td>0.3300</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.753454</td>\n",
       "      <td>0.248640</td>\n",
       "      <td>158.695024</td>\n",
       "      <td>0.388500</td>\n",
       "      <td>247.960975</td>\n",
       "      <td>408.481819</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.031891</td>\n",
       "      <td>IT-PT1</td>\n",
       "      <td>DBF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-27</th>\n",
       "      <td>0.929902</td>\n",
       "      <td>232.271396</td>\n",
       "      <td>0.1132</td>\n",
       "      <td>0.2175</td>\n",
       "      <td>0.323333</td>\n",
       "      <td>0.383333</td>\n",
       "      <td>0.315392</td>\n",
       "      <td>0.068598</td>\n",
       "      <td>15.933277</td>\n",
       "      <td>0.212158</td>\n",
       "      <td>49.278176</td>\n",
       "      <td>75.101085</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.012382</td>\n",
       "      <td>ES-Amo</td>\n",
       "      <td>OSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-28</th>\n",
       "      <td>1.053588</td>\n",
       "      <td>232.694833</td>\n",
       "      <td>0.1133</td>\n",
       "      <td>0.2176</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.315201</td>\n",
       "      <td>0.068588</td>\n",
       "      <td>15.960011</td>\n",
       "      <td>0.216593</td>\n",
       "      <td>50.400033</td>\n",
       "      <td>73.686697</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>0.014298</td>\n",
       "      <td>ES-Amo</td>\n",
       "      <td>OSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-29</th>\n",
       "      <td>0.828637</td>\n",
       "      <td>230.206604</td>\n",
       "      <td>0.1120</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.316239</td>\n",
       "      <td>0.068181</td>\n",
       "      <td>15.695762</td>\n",
       "      <td>0.219939</td>\n",
       "      <td>50.631489</td>\n",
       "      <td>71.364047</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.011611</td>\n",
       "      <td>ES-Amo</td>\n",
       "      <td>OSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-30</th>\n",
       "      <td>0.935563</td>\n",
       "      <td>216.623437</td>\n",
       "      <td>0.1122</td>\n",
       "      <td>0.2162</td>\n",
       "      <td>0.303333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.316687</td>\n",
       "      <td>0.068468</td>\n",
       "      <td>14.831713</td>\n",
       "      <td>0.225718</td>\n",
       "      <td>48.895758</td>\n",
       "      <td>65.709109</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.014238</td>\n",
       "      <td>ES-Amo</td>\n",
       "      <td>OSH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-31</th>\n",
       "      <td>0.887919</td>\n",
       "      <td>178.998667</td>\n",
       "      <td>0.1131</td>\n",
       "      <td>0.2187</td>\n",
       "      <td>0.296667</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>0.318264</td>\n",
       "      <td>0.069604</td>\n",
       "      <td>12.459084</td>\n",
       "      <td>0.234621</td>\n",
       "      <td>41.996913</td>\n",
       "      <td>53.102938</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.016721</td>\n",
       "      <td>ES-Amo</td>\n",
       "      <td>OSH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>318168 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  gpp         par     red     nir      fpar       lai  \\\n",
       "2002-07-04  12.562600  642.650645  0.0463  0.3376  0.660000  1.800000   \n",
       "2002-07-05  14.247250  554.901392  0.0464  0.3308  0.655000  1.800000   \n",
       "2002-07-06  10.387950  351.058726  0.0463  0.3307  0.650000  1.800000   \n",
       "2002-07-07  13.161150  656.717066  0.0462  0.3306  0.645000  1.800000   \n",
       "2002-07-08  13.027000  638.252843  0.0464  0.3300  0.640000  1.800000   \n",
       "...               ...         ...     ...     ...       ...       ...   \n",
       "2012-12-27   0.929902  232.271396  0.1132  0.2175  0.323333  0.383333   \n",
       "2012-12-28   1.053588  232.694833  0.1133  0.2176  0.316667  0.366667   \n",
       "2012-12-29   0.828637  230.206604  0.1120  0.2156  0.310000  0.350000   \n",
       "2012-12-30   0.935563  216.623437  0.1122  0.2162  0.303333  0.333333   \n",
       "2012-12-31   0.887919  178.998667  0.1131  0.2187  0.296667  0.316667   \n",
       "\n",
       "                ndvi      nirv       nirvp      fesc      fesc_p        apar  \\\n",
       "2002-07-04  0.758791  0.256168  164.626505  0.388133  249.434098  424.149425   \n",
       "2002-07-05  0.753977  0.249415  138.400998  0.380787  211.299234  363.460412   \n",
       "2002-07-06  0.754377  0.249472   87.579449  0.383804  134.737614  228.188172   \n",
       "2002-07-07  0.754777  0.249529  163.870149  0.386867  254.062247  423.582507   \n",
       "2002-07-08  0.753454  0.248640  158.695024  0.388500  247.960975  408.481819   \n",
       "...              ...       ...         ...       ...         ...         ...   \n",
       "2012-12-27  0.315392  0.068598   15.933277  0.212158   49.278176   75.101085   \n",
       "2012-12-28  0.315201  0.068588   15.960011  0.216593   50.400033   73.686697   \n",
       "2012-12-29  0.316239  0.068181   15.695762  0.219939   50.631489   71.364047   \n",
       "2012-12-30  0.316687  0.068468   14.831713  0.225718   48.895758   65.709109   \n",
       "2012-12-31  0.318264  0.069604   12.459084  0.234621   41.996913   53.102938   \n",
       "\n",
       "              fesc_n       lue    name type  \n",
       "2002-07-04  0.000604  0.029618  IT-PT1  DBF  \n",
       "2002-07-05  0.000686  0.039199  IT-PT1  DBF  \n",
       "2002-07-06  0.001093  0.045524  IT-PT1  DBF  \n",
       "2002-07-07  0.000589  0.031071  IT-PT1  DBF  \n",
       "2002-07-08  0.000609  0.031891  IT-PT1  DBF  \n",
       "...              ...       ...     ...  ...  \n",
       "2012-12-27  0.000913  0.012382  ES-Amo  OSH  \n",
       "2012-12-28  0.000931  0.014298  ES-Amo  OSH  \n",
       "2012-12-29  0.000955  0.011611  ES-Amo  OSH  \n",
       "2012-12-30  0.001042  0.014238  ES-Amo  OSH  \n",
       "2012-12-31  0.001311  0.016721  ES-Amo  OSH  \n",
       "\n",
       "[318168 rows x 16 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding clumping index values (if asked by reviewer later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ci = xr.open_dataset(\n",
    "#     \"../data/EC/Global_Clumping_Index_1531/data/global_clumping_index_geographic.nc\"\n",
    "# )\n",
    "# ci = ci.__xarray_dataarray_variable__\n",
    "# ameriflux_coords = pd.read_csv(\"../data/Ameriflux_coords.csv\")\n",
    "# fluxnet_coords = pd.read_csv(\"../data/Fluxnet_coords.csv\")\n",
    "# merged_coords = pd.concat([ameriflux_coords, fluxnet_coords], ignore_index=True)\n",
    "# merged_coords.drop_duplicates(subset=merged_coords.columns[0], inplace=True)\n",
    "# merged_coords.reset_index(drop=True, inplace=True)\n",
    "# merged_coords.rename({\"Name\": \"name\"}, axis=1, inplace=True)\n",
    "# pd_all = pd.merge(df, merged_coords[[\"name\", \"Lat\", \"Lon\"]], on=\"name\", how=\"left\")\n",
    "# pd_all.set_index(df.index, inplace=True)\n",
    "# names = pd_all[\"name\"].unique()\n",
    "# for name in names:\n",
    "#     lat = pd_all[pd_all[\"name\"] == name][\"Lat\"].values[0]\n",
    "#     lon = pd_all[pd_all[\"name\"] == name][\"Lon\"].values[0]\n",
    "#     ci_point = ci.sel(x=lon, y=lat, method=\"nearest\").values\n",
    "#     pd_all.loc[pd_all[\"name\"] == name, \"ci\"] = ci_point\n",
    "# pd_all.to_csv(\"../outputs/data_clean_fpar_lai_ci2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dscovr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
