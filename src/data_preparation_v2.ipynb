{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from scipy.stats import zscore, linregress\n",
    "from pandas.plotting import scatter_matrix\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "def remove_outliers(df, threshold=3):\n",
    "    # Initialize a boolean mask to keep track of rows to drop\n",
    "    outlier_rows_mask = np.zeros(len(df), dtype=bool)\n",
    "\n",
    "    # Iterate over each column\n",
    "    for col in df.columns:\n",
    "        # Skip the \"t1\" and \"t2\" columns\n",
    "        if col == \"t1\" or col == \"t2\":\n",
    "            continue\n",
    "\n",
    "        # Calculate the mean and standard deviation of the column\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "\n",
    "        # Find outliers in this column\n",
    "        outliers = (df[col] - mean).abs() > threshold * std\n",
    "\n",
    "        # Mark rows with outliers in this column\n",
    "        outlier_rows_mask = np.logical_or(outlier_rows_mask, outliers)\n",
    "\n",
    "    # Drop rows with outliers\n",
    "    cleaned_df = df[~outlier_rows_mask]\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluxnet_info = pd.read_csv(\"../data/EC/fluxnet/sites_info.csv\")\n",
    "ameriflux_info = pd.read_csv(\"../data/EC/Ameriflux/sites_info.tsv\", delimiter=\"\\t\")\n",
    "fluxnet_names = fluxnet_info[\"ID\"].to_list()\n",
    "fluxnet_types = fluxnet_info[\"type\"].to_list()\n",
    "ameriflux_names = ameriflux_info[\"Site ID\"].to_list()\n",
    "ameriflux_types = ameriflux_info[\"Vegetation Abbreviation (IGBP)\"].to_list()\n",
    "\n",
    "combined_names = list(set(ameriflux_names + fluxnet_names))\n",
    "combined_types = []\n",
    "for name in combined_names:\n",
    "    if name in ameriflux_names and name in fluxnet_names:\n",
    "        # Choose a type from either fluxnet_types or ameriflux_types\n",
    "        combined_types.append(fluxnet_types[fluxnet_names.index(name)])\n",
    "    elif name in ameriflux_names:\n",
    "        combined_types.append(ameriflux_types[ameriflux_names.index(name)])\n",
    "    else:\n",
    "        combined_types.append(fluxnet_types[fluxnet_names.index(name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ec = []\n",
    "\n",
    "for i in range(len(combined_names)):\n",
    "    site_name = combined_names[i]\n",
    "    site_type = combined_types[i]\n",
    "\n",
    "    if site_name in ameriflux_names:\n",
    "        file = glob.glob(\"../data/EC/Ameriflux/AMF_\" + site_name + \"*DD*\")\n",
    "    else:\n",
    "        file = glob.glob(\"../data/EC/fluxnet/FLX_\" + site_name + \"*DD*\")\n",
    "\n",
    "    ec = pd.read_csv(file[0])\n",
    "    ec.loc[:, \"type\"] = site_type\n",
    "    ec.loc[:, \"name\"] = site_name\n",
    "    ec.index = pd.to_datetime(ec[\"TIMESTAMP\"], format=\"%Y%m%d\")\n",
    "    combined_ec.append(ec)\n",
    "combined_ec = pd.concat(combined_ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for four day fpar\n",
    "MCD43_fluxnet = []\n",
    "MCD15_fluxnet = []\n",
    "MCD43_ameriflux = []\n",
    "MCD15_ameriflux = []\n",
    "\n",
    "# Loop over batches (#5) of downloaded data\n",
    "for i in range(1, 5):\n",
    "    refl_fluxnet = glob.glob(\n",
    "        \"../data/EC/fluxnet/sat_data/*batch\" + str(i) + \"*MCD43A4-061-results.csv\"\n",
    "    )\n",
    "    sat_refl_fluxnet = pd.read_csv(refl_fluxnet[0])\n",
    "    sat_refl_fluxnet.loc[:, \"time\"] = pd.to_datetime(sat_refl_fluxnet[\"Date\"])\n",
    "    sat_refl_fluxnet.set_index(sat_refl_fluxnet[\"Date\"], inplace=True)\n",
    "    MCD43_fluxnet.append(sat_refl_fluxnet)\n",
    "\n",
    "    fpar_fluxnet = glob.glob(\n",
    "        \"../data/EC/fluxnet/sat_data/*batch\" + str(i) + \"*MCD15A3H-061-results.csv\"\n",
    "    )\n",
    "    sat_fpar_fluxnet = pd.read_csv(fpar_fluxnet[0])\n",
    "    sat_fpar_fluxnet.loc[:, \"time\"] = pd.to_datetime(sat_fpar_fluxnet[\"Date\"])\n",
    "    sat_fpar_fluxnet.set_index(sat_fpar_fluxnet[\"Date\"], inplace=True)\n",
    "    MCD15_fluxnet.append(sat_fpar_fluxnet)\n",
    "\n",
    "    # Note all the ameriflux sites are in 4 batches\n",
    "    if i < 5:\n",
    "        refl_ameriflux = glob.glob(\n",
    "            \"../data/EC/Ameriflux/sat_data/*batch\" + str(i) + \"*MCD43A4-061-results.csv\"\n",
    "        )\n",
    "\n",
    "        sat_refl_ameriflux = pd.read_csv(refl_ameriflux[0])\n",
    "        sat_refl_ameriflux.loc[:, \"time\"] = pd.to_datetime(sat_refl_ameriflux[\"Date\"])\n",
    "        sat_refl_ameriflux.set_index(sat_refl_ameriflux[\"Date\"], inplace=True)\n",
    "        MCD43_ameriflux.append(sat_refl_ameriflux)\n",
    "\n",
    "        fpar_ameriflux = glob.glob(\n",
    "            \"../data/EC/Ameriflux/sat_data/*batch\"\n",
    "            + str(i)\n",
    "            + \"*MCD15A3H-061-results.csv\"\n",
    "        )\n",
    "        fpar_ameriflux = pd.read_csv(fpar_ameriflux[0])\n",
    "\n",
    "        fpar_ameriflux.loc[:, \"time\"] = pd.to_datetime(fpar_ameriflux[\"Date\"])\n",
    "        fpar_ameriflux.set_index(fpar_ameriflux[\"Date\"], inplace=True)\n",
    "        MCD15_ameriflux.append(fpar_ameriflux)\n",
    "\n",
    "\n",
    "refl_fluxnet = pd.concat(MCD43_fluxnet)\n",
    "refl_fluxnet = refl_fluxnet.rename(columns={\"ID\": \"name\"})\n",
    "\n",
    "fpar_fluxnet = pd.concat(MCD15_fluxnet)\n",
    "fpar_fluxnet = fpar_fluxnet.rename(columns={\"ID\": \"name\"})\n",
    "\n",
    "refl_ameriflux = pd.concat(MCD43_ameriflux)\n",
    "refl_ameriflux = refl_ameriflux.rename(columns={\"ID\": \"name\"})\n",
    "fpar_ameriflux = pd.concat(MCD15_ameriflux)\n",
    "fpar_ameriflux = fpar_ameriflux.rename(columns={\"ID\": \"name\"})\n",
    "combined_refl = []\n",
    "combined_fpar = []\n",
    "\n",
    "for name in combined_names:\n",
    "    if name in ameriflux_names:\n",
    "        selected_refl = refl_ameriflux[refl_ameriflux[\"name\"] == name]\n",
    "        selected_fpar = fpar_ameriflux[fpar_ameriflux[\"name\"] == name]\n",
    "    else:\n",
    "        selected_refl = refl_fluxnet[refl_fluxnet[\"name\"] == name]\n",
    "        selected_fpar = fpar_fluxnet[fpar_fluxnet[\"name\"] == name]\n",
    "\n",
    "    combined_refl.append(selected_refl)\n",
    "    combined_fpar.append(selected_fpar)\n",
    "\n",
    "combined_refl = pd.concat(combined_refl)\n",
    "combined_fpar = pd.concat(combined_fpar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flags come from [This paper](https://pdf.sciencedirectassets.com/271723/1-s2.0-S0168192320X00062/1-s2.0-S0168192320301945/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDsaCXVzLWVhc3QtMSJGMEQCIH4OB25%2B%2BrWEsBRZZ4CMPUgYETlwWwu0kcC2JF5q5VHKAiAysHZntlDvw8izOPtHcQa1YDDXXdQ%2ByZrXPCaxn2NdLiqzBQg0EAUaDDA1OTAwMzU0Njg2NSIMhywMz6eScMTpU2DpKpAFhCNp4JUVE3sxgm2Ly4FDKc%2BSr7igdIpyk9TsBrE%2F0lC51AZ4N3waj2MpXB%2BA2dorxigw%2BZqxNIgin%2BQFdFvFUN0k6mgrcLTBkS9FhRXOMtTKHvqFgR6FAHKZynNnFuxkIM6eV3dZLRHS0R2yeyRpHxGUk%2FYdd6MCozWZKdmaO00mNuMaCQNgwifIIBvwKStqkc9WTys%2F0PXrBO48pSfm90AcEbBzjGiJRgmIpoKoW%2BUkwvBmd%2ByoyK9%2FIQ8nHz9nN7g%2FgopA5nBLXDRvT29mo0D3nFjO6of%2Fm0aPVA0cX2OCmbYDrdb4s%2BTl%2Fb3Cx0HBquT78mhkYTCbOd6YxFHpmb1s6QDcD%2B%2Bl003yqSDYyJdzZmoq0D8wI8NQWJcycDEPI5fgCYxaS0WubAL2QPYO4u%2BOopqWqBfe1l4gf7nkEe8Pp5UAe2vjYGxV5mN9dkOOhlrKXpWa697KXIATqSoYGQwrkumPECnPQWs3FFVrwDiwWDJNvUqUikDqIFG1zFGD7xwzLQh%2BWfrKyxhi%2BfiE8rY4YlFVQ2e6M4DwrmwPtkagsRtzufygm1nJ8bF9z%2F3aiyioMBwCeQ9y3zWT42L5V%2BRtf95N0aGjjjlfzKMB04euz6YWqkhoVuaKGo2WFlt82S4B9PPxa8d8v3bTwxZgfY%2FAXJaAsbET7i9t89h9XF9aZmjR06YbPLi4F0%2FMAbq0njAWnW6oSW9xYXl7Rugx1p8DpKKtzyBr%2FpzNj9djuFAfKUnfLOuiu8rZpsPctz%2FDKwchAqjiYLWtPsTo%2FuyIhZoDBcO3KujlTP7F4VbI87xj5JFXwdsw5C1ZrP0b8N9JZZsZO0V9yhUS8n07bIc3grqIS%2Fn7wzsBGvHu5jiqpMIwz8SIrwY6sgHXwF3s5SpzUmT%2Bd%2FpaOxFaSWK69qbq1fh%2Bv92O3hmad2L%2F82iL5dKCVYcfn5cEbl1mCCJGLbj%2B1JACCSn%2Bb0IDzbqm9PlhBh09lYsoQFVr%2FMaOCh2jlc6UJX%2BWuWuVQ1DercdbawxGlkLwfrDweiqdswRdEHNP5yU6LkedJVvBuG%2Ffm4Kl05DMaCUpKB5zH0b04M16ASPcw5wiPUpNMuyudj1Ko4wwx2sOZt%2BDxbVJHEdP&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240301T194332Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYY7GY5ACN%2F20240301%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=746656b69aa01f343be150d203fa4d604f6da3aad6570344a69b5eda22f1ac03&hash=da2637ff2882758ae7d5d5d82b08ed3897533c9ea8fae448ed1b18c618c8b49a&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0168192320301945&tid=spdf-f3eb7708-9f66-472e-82f4-ba59d2040f74&sid=54e759056934f749ce7937c-05135ec4bd54gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0f1557550402015c085500&rr=85dba2f5b9a06a41&cc=us)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 IT-La2 ENF  site_df\n",
      "20 DE-RuS CRO  fpar\n",
      "21 DE-Akm WET  fpar\n",
      "24 US-KS3 WET  fpar\n",
      "57 IT-Ro1 DBF  fpar\n",
      "58 US-WPT WET  fpar\n",
      "95 US-LWW GRA  site_df\n",
      "97 US-Pnp WAT  fpar\n",
      "105 ZM-Mon DBF  fpar\n",
      "174 US-HB1 WET  fpar\n",
      "193 US-Me4 ENF  site_df\n",
      "227 GH-Ank EBF  site_df\n",
      "231 US-Snf GRA  fpar\n",
      "248 US-UM3 WAT  fpar\n",
      "262 CG-Tch SAV  site_df\n",
      "287 US-ORv WET  fpar\n"
     ]
    }
   ],
   "source": [
    "par_counter = 0\n",
    "combined_data = []\n",
    "for i in range(len(combined_names)):\n",
    "    name = combined_names[i]\n",
    "    type = combined_types[i]\n",
    "    ec_data = combined_ec[combined_ec[\"name\"] == name]\n",
    "    # # -------------Filter GPP------------------------\n",
    "    # # The gap filled data is less than 20%\n",
    "    ec_data = ec_data[ec_data[\"NEE_VUT_REF_QC\"] > 0.8]\n",
    "\n",
    "    # Uncertaninity in NEE is less than 3  C m−2 d−1\n",
    "    ec_data = ec_data[ec_data[\"NEE_VUT_REF_RANDUNC\"] < 3]\n",
    "\n",
    "    # Difference between GPP day and night is less than 3 C m−2 d−1\n",
    "    # idx_good_gpp = abs(ec_data[\"GPP_DT_VUT_REF\"] - ec_data[\"GPP_NT_VUT_REF\"]) < 3\n",
    "    # gpp_day = ec_data[\"GPP_DT_VUT_REF\"][idx_good_gpp]\n",
    "    # gpp_night = ec_data[\"GPP_NT_VUT_REF\"][idx_good_gpp]\n",
    "    gpp_night = ec_data[\"GPP_NT_VUT_REF\"]\n",
    "    gpp_day = ec_data[\"GPP_DT_VUT_REF\"]\n",
    "\n",
    "    # # Take the mean of the day and night GPP\n",
    "    gpp = (gpp_day + gpp_night) / 2\n",
    "\n",
    "    # Make sure there is no negative GPP\n",
    "    gpp = gpp[gpp > 0]\n",
    "\n",
    "    # ----------------Filter PAR-----------------------\n",
    "    par_qc = ec_data.loc[gpp.index, \"PPFD_IN_QC\"]\n",
    "    sw_qc = ec_data.loc[gpp.index, \"SW_IN_F_QC\"]\n",
    "\n",
    "    idx_good_par = par_qc[par_qc > 0.8]\n",
    "    idx_good_sw = sw_qc[sw_qc > 0.8]\n",
    "\n",
    "    par = ec_data.loc[idx_good_par.index, \"PPFD_IN\"]\n",
    "    sw = ec_data.loc[idx_good_sw.index, \"SW_IN_F\"]\n",
    "\n",
    "    # # Alternative way to calculate PAR\n",
    "    if par.empty:\n",
    "        # convert SW from w m-2 to umol m-2 s-1 then multiply by 0.45 to get PAR\n",
    "        par = sw * 4.57 * 0.45\n",
    "        par_counter += 1\n",
    "\n",
    "    gpp = gpp[par.index]\n",
    "    gpp = gpp.to_frame(\"gpp\")\n",
    "    par = par.to_frame(\"par\")\n",
    "\n",
    "    if par.empty or gpp.empty:\n",
    "        print(i, name, type, \"Par or GPP is empty\")\n",
    "        continue\n",
    "\n",
    "    site_ec = pd.concat([gpp, par], axis=1)\n",
    "\n",
    "    # ----------------Filter Reflectance and FPAR-----------------------\n",
    "    site_refl = combined_refl[combined_refl[\"name\"] == name]\n",
    "\n",
    "    site_refl.index = pd.to_datetime(site_refl.index, format=\"%Y-%m-%d\")\n",
    "    site_refl = site_refl[site_refl.index.isin(site_ec.index)]\n",
    "    filtered_refl = site_refl[\n",
    "        (\n",
    "            site_refl[\"MCD43A4_061_BRDF_Albedo_Band_Mandatory_Quality_Band1_MODLAND\"]\n",
    "            == \"0b000\"\n",
    "        )\n",
    "        & (\n",
    "            site_refl[\"MCD43A4_061_BRDF_Albedo_Band_Mandatory_Quality_Band2_MODLAND\"]\n",
    "            == \"0b000\"\n",
    "        )\n",
    "    ].copy()\n",
    "\n",
    "    site_red = filtered_refl[[\"MCD43A4_061_Nadir_Reflectance_Band1\"]].rename(\n",
    "        columns={\"MCD43A4_061_Nadir_Reflectance_Band1\": \"red\"}\n",
    "    )\n",
    "    site_nir = filtered_refl[[\"MCD43A4_061_Nadir_Reflectance_Band2\"]].rename(\n",
    "        columns={\"MCD43A4_061_Nadir_Reflectance_Band2\": \"nir\"}\n",
    "    )\n",
    "\n",
    "    site_fpar = combined_fpar[combined_fpar[\"name\"] == name]\n",
    "    site_fpar.index = pd.to_datetime(site_fpar.index, format=\"%Y-%m-%d\")\n",
    "\n",
    "    filtered_fpar = site_fpar[\n",
    "        (site_fpar[\"MCD15A3H_061_FparLai_QC_MODLAND\"] == \"0b0\")\n",
    "        & (site_fpar[\"MCD15A3H_061_FparLai_QC_DeadDetector\"] == \"0b0\")\n",
    "        & (site_fpar[\"MCD15A3H_061_FparLai_QC_CloudState\"] == \"0b00\")\n",
    "        & (site_fpar[\"MCD15A3H_061_FparLai_QC_SCF_QC\"].isin([\"0b000\", \"0b001\"]))\n",
    "    ].copy()\n",
    "    if filtered_fpar.empty:\n",
    "        print(i, name, type, \" fpar\")\n",
    "        continue\n",
    "    fpar_tmp = filtered_fpar[\"MCD15A3H_061_Fpar_500m\"]\n",
    "    site_fpar = fpar_tmp.resample(\"D\").interpolate(\"linear\")\n",
    "    site_fpar = site_fpar.to_frame(\"fpar\")\n",
    "    site_fpar = site_fpar[site_fpar.index.isin(site_ec.index)]\n",
    "\n",
    "    lai_tmp = filtered_fpar[\"MCD15A3H_061_Lai_500m\"]\n",
    "    site_lai = lai_tmp.resample(\"D\").interpolate(\"linear\")\n",
    "    site_lai = site_lai.to_frame(\"lai\")\n",
    "    site_lai = site_lai[site_lai.index.isin(site_ec.index)]\n",
    "    # Merge the dataframes\n",
    "    site_df = (\n",
    "        site_ec.merge(site_red, left_index=True, right_index=True)\n",
    "        .merge(site_nir, left_index=True, right_index=True)\n",
    "        .merge(site_fpar, left_index=True, right_index=True)\n",
    "        .merge(site_lai, left_index=True, right_index=True)\n",
    "    )\n",
    "    if site_df.empty:\n",
    "        print(i, name, type, \" site_df\")\n",
    "        continue\n",
    "    # Calculate the NDVI, NIRv, NIRvp, Fesc, and LUE\n",
    "    site_df.loc[:, \"ndvi\"] = (site_df[\"nir\"] - site_df[\"red\"]) / (\n",
    "        site_df[\"nir\"] + site_df[\"red\"]\n",
    "    )\n",
    "    site_df.loc[:, \"nirv\"] = site_df[\"ndvi\"] * site_df[\"nir\"]\n",
    "    site_df.loc[:, \"nirvp\"] = site_df[\"nirv\"] * site_df[\"par\"]\n",
    "    site_df.loc[:, \"fesc\"] = site_df[\"nirv\"] / site_df[\"fpar\"]\n",
    "    site_df.loc[:, \"fesc_p\"] = site_df[\"fesc\"] * site_df[\"par\"]\n",
    "    site_df.loc[:, \"fesc_n\"] = site_df[\"nirv\"] / (site_df[\"fpar\"] * site_df[\"par\"])\n",
    "\n",
    "    site_df.loc[:, \"lue\"] = site_df[\"gpp\"] / (site_df[\"par\"] * site_df[\"fpar\"])\n",
    "    site_df = site_df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    cleaned_site_df = remove_outliers(site_df, 3).copy()\n",
    "    cleaned_site_df.loc[:, \"name\"] = name\n",
    "    cleaned_site_df.loc[:, \"type\"] = type\n",
    "    combined_data.append(cleaned_site_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('number of sites with par being approximated:', 60)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"number of sites with par being approximated:\", par_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"type\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the GLASS LAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 112] Host is down: '/home/hamid/mnt/nas/Hamid/GLASS/csv_files/FR-LBr.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m result_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m names:\n\u001b[0;32m----> 8\u001b[0m     glass_lai \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(glass_dir \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m glass_lai\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: No Glass LAI\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dscovr/lib/python3.12/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/dscovr/lib/python3.12/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/dscovr/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/envs/dscovr/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/dscovr/lib/python3.12/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 112] Host is down: '/home/hamid/mnt/nas/Hamid/GLASS/csv_files/FR-LBr.csv'"
     ]
    }
   ],
   "source": [
    "glass_dir = \"/home/hamid/mnt/nas/Hamid/GLASS/csv_files/\"\n",
    "names = df[\"name\"].unique()\n",
    "\n",
    "# Initialize an empty DataFrame to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "for name in names:\n",
    "    glass_lai = pd.read_csv(glass_dir + name + \".csv\")\n",
    "    if glass_lai.empty:\n",
    "        print(name, \": No Glass LAI\")\n",
    "        continue\n",
    "    glass_lai.set_index(\"Unnamed: 0\", inplace=True)\n",
    "    glass_lai.index = pd.to_datetime(glass_lai.index, format=\"%Y-%m-%d\")\n",
    "    glass_lai_daily = glass_lai.resample(\"D\").interpolate(\"linear\")\n",
    "    glass_lai_daily.rename(columns={\"LAI\": \"glass_lai\"}, inplace=True)\n",
    "    glass_lai_daily.index.name = \"\"\n",
    "    tmp_df = df[df[\"name\"] == name]\n",
    "    tmp_df = tmp_df.merge(glass_lai_daily, left_index=True, right_index=True)\n",
    "\n",
    "    # Append the tmp_df to the result_df\n",
    "    result_df = pd.concat([result_df, tmp_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"../outputs/data_clean_glass_lai.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding clumping index values (if asked by reviewer later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = xr.open_dataset(\n",
    "    \"../data/EC/Global_Clumping_Index_1531/data/global_clumping_index_geographic.nc\"\n",
    ")\n",
    "ci = ci.__xarray_dataarray_variable__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ameriflux_coords = pd.read_csv(\"../data/Ameriflux_coords.csv\")\n",
    "fluxnet_coords = pd.read_csv(\"../data/Fluxnet_coords.csv\")\n",
    "merged_coords = pd.concat([ameriflux_coords, fluxnet_coords], ignore_index=True)\n",
    "merged_coords.drop_duplicates(subset=merged_coords.columns[0], inplace=True)\n",
    "merged_coords.reset_index(drop=True, inplace=True)\n",
    "merged_coords.rename({\"Name\": \"name\"}, axis=1, inplace=True)\n",
    "pd_all = pd.merge(df, merged_coords[[\"name\", \"Lat\", \"Lon\"]], on=\"name\", how=\"left\")\n",
    "pd_all.set_index(df.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd_all[\"name\"].unique()\n",
    "for name in names:\n",
    "    lat = pd_all[pd_all[\"name\"] == name][\"Lat\"].values[0]\n",
    "    lon = pd_all[pd_all[\"name\"] == name][\"Lon\"].values[0]\n",
    "    ci_point = ci.sel(x=lon, y=lat, method=\"nearest\").values\n",
    "    pd_all.loc[pd_all[\"name\"] == name, \"ci\"] = ci_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_all.to_csv(\"../outputs/data_clean_fpar_lai_ci2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dscovr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
