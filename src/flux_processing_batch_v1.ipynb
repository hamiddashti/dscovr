{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from scipy.stats import zscore, linregress\n",
    "from pandas.plotting import scatter_matrix\n",
    "from scipy import stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove outliers from a dataframe\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "def remove_outliers(df):\n",
    "    # Initialize a boolean mask to keep track of rows to drop\n",
    "    outlier_rows_mask = np.zeros(len(df), dtype=bool)\n",
    "\n",
    "    # Iterate over each column\n",
    "    for col in df.columns:\n",
    "        # Fit the LocalOutlierFactor model to the column data\n",
    "        lof = LocalOutlierFactor()\n",
    "        outliers = lof.fit_predict(df[col].values.reshape(-1, 1))\n",
    "\n",
    "        # Mark rows with outliers in this column\n",
    "        outlier_rows_mask = np.logical_or(outlier_rows_mask, outliers == -1)\n",
    "\n",
    "    # Drop rows with outliers\n",
    "    # cleaned_df = df[~outlier_rows_mask]\n",
    "    return outlier_rows_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluxnet_info = pd.read_csv(\"../data/EC/fluxnet/sites_info.csv\")\n",
    "ameriflux_info = pd.read_csv(\"../data/EC/Ameriflux/sites_info.tsv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluxnet_names = fluxnet_info[\"ID\"].to_list()\n",
    "fluxnet_types = fluxnet_info[\"type\"].to_list()\n",
    "ameriflux_names = ameriflux_info[\"Site ID\"].to_list()\n",
    "ameriflux_types = ameriflux_info[\"Vegetation Abbreviation (IGBP)\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_names = list(set(ameriflux_names + fluxnet_names))\n",
    "combined_types = []\n",
    "for name in combined_names:\n",
    "    if name in ameriflux_names and name in fluxnet_names:\n",
    "        # Choose a type from either fluxnet_types or ameriflux_types\n",
    "        combined_types.append(fluxnet_types[fluxnet_names.index(name)])\n",
    "    elif name in ameriflux_names:\n",
    "        combined_types.append(ameriflux_types[ameriflux_names.index(name)])\n",
    "    else:\n",
    "        combined_types.append(fluxnet_types[fluxnet_names.index(name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "# Iterate over the range of length of 'name' column in sites_info\n",
    "for i in range(len(combined_names)):\n",
    "    site_name = combined_names[i]\n",
    "    site_type = combined_types[i]\n",
    "\n",
    "    # Set the file path based on whether the combined name is in ameriflux_names or fluxnet_names\n",
    "    if site_name in ameriflux_names:\n",
    "        file = glob.glob(\"../data/EC/Ameriflux/AMF_\" + site_name + \"*DD*\")\n",
    "    else:\n",
    "        file = glob.glob(\"../data/EC/fluxnet/FLX_\" + site_name + \"*DD*\")\n",
    "\n",
    "    # Open the CSV file\n",
    "    ec = pd.read_csv(file[0])\n",
    "    ec.loc[:, \"type\"] = site_type\n",
    "    ec.loc[:, \"name\"] = site_name\n",
    "\n",
    "    ec[\"t\"] = pd.to_datetime(ec[\"TIMESTAMP\"], format=\"%Y%m%d\")\n",
    "    ec = ec.set_index(\"t\")\n",
    "\n",
    "    # Append the DataFrame to the list\n",
    "    dfs.append(ec)\n",
    "\n",
    "# Concatenate all DataFrames in the list\n",
    "combined_ec = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all the satellite data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCD43_fluxnet = []\n",
    "MCD15_fluxnet = []\n",
    "MCD43_ameriflux = []\n",
    "MCD15_ameriflux = []\n",
    "\n",
    "# Loop over batches (#5) of downloaded data\n",
    "for i in range(1, 5):\n",
    "    refl_fluxnet = glob.glob(\n",
    "        \"../data/EC/fluxnet/sat_data/*batch\" + str(i) + \"*MCD43A4-061-results.csv\"\n",
    "    )\n",
    "    sat_refl_fluxnet = pd.read_csv(refl_fluxnet[0])\n",
    "    sat_refl_fluxnet.loc[:, \"time\"] = pd.to_datetime(sat_refl_fluxnet[\"Date\"])\n",
    "    sat_refl_fluxnet.set_index(sat_refl_fluxnet[\"Date\"], inplace=True)\n",
    "    MCD43_fluxnet.append(sat_refl_fluxnet)\n",
    "\n",
    "    fpar_fluxnet = glob.glob(\n",
    "        \"../data/EC/fluxnet/sat_data/*batch\" + str(i) + \"*MCD15A3H-061-results.csv\"\n",
    "    )\n",
    "    sat_fpar_fluxnet = pd.read_csv(fpar_fluxnet[0])\n",
    "    sat_fpar_fluxnet.loc[:, \"time\"] = pd.to_datetime(sat_fpar_fluxnet[\"Date\"])\n",
    "    sat_fpar_fluxnet.set_index(sat_fpar_fluxnet[\"Date\"], inplace=True)\n",
    "    MCD15_fluxnet.append(sat_fpar_fluxnet)\n",
    "\n",
    "    if i < 5:\n",
    "        refl_ameriflux = glob.glob(\n",
    "            \"../data/EC/Ameriflux/sat_data/*batch\" + str(i) + \"*MCD43A4-061-results.csv\"\n",
    "        )\n",
    "\n",
    "        sat_refl_ameriflux = pd.read_csv(refl_ameriflux[0])\n",
    "        sat_refl_ameriflux.loc[:, \"time\"] = pd.to_datetime(sat_refl_ameriflux[\"Date\"])\n",
    "        sat_refl_ameriflux.set_index(sat_refl_ameriflux[\"Date\"], inplace=True)\n",
    "        MCD43_ameriflux.append(sat_refl_ameriflux)\n",
    "\n",
    "        fpar_ameriflux = glob.glob(\n",
    "            \"../data/EC/Ameriflux/sat_data/*batch\"\n",
    "            + str(i)\n",
    "            + \"*MCD15A3H-061-results.csv\"\n",
    "        )\n",
    "        fpar_ameriflux = pd.read_csv(fpar_ameriflux[0])\n",
    "\n",
    "        fpar_ameriflux.loc[:, \"time\"] = pd.to_datetime(fpar_ameriflux[\"Date\"])\n",
    "        fpar_ameriflux.set_index(fpar_ameriflux[\"Date\"], inplace=True)\n",
    "        MCD15_ameriflux.append(fpar_ameriflux)\n",
    "\n",
    "\n",
    "refl_fluxnet = pd.concat(MCD43_fluxnet)\n",
    "refl_fluxnet = refl_fluxnet.rename(columns={\"ID\": \"name\"})\n",
    "\n",
    "fpar_fluxnet = pd.concat(MCD15_fluxnet)\n",
    "fpar_fluxnet = fpar_fluxnet.rename(columns={\"ID\": \"name\"})\n",
    "\n",
    "refl_ameriflux = pd.concat(MCD43_ameriflux)\n",
    "refl_ameriflux = refl_ameriflux.rename(columns={\"ID\": \"name\"})\n",
    "fpar_ameriflux = pd.concat(MCD15_ameriflux)\n",
    "fpar_ameriflux = fpar_ameriflux.rename(columns={\"ID\": \"name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_refl = []\n",
    "combined_fpar = []\n",
    "\n",
    "for name in combined_names:\n",
    "    if name in ameriflux_names:\n",
    "        selected_refl = refl_ameriflux[refl_ameriflux[\"name\"] == name]\n",
    "        selected_fpar = fpar_ameriflux[fpar_ameriflux[\"name\"] == name]\n",
    "    else:\n",
    "        selected_refl = refl_fluxnet[refl_fluxnet[\"name\"] == name]\n",
    "        selected_fpar = fpar_fluxnet[fpar_fluxnet[\"name\"] == name]\n",
    "\n",
    "    combined_refl.append(selected_refl)\n",
    "    combined_fpar.append(selected_fpar)\n",
    "\n",
    "combined_refl = pd.concat(combined_refl)\n",
    "combined_fpar = pd.concat(combined_fpar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sites_list = []\n",
    "df_stat = []\n",
    "df_stat_pft = []\n",
    "\n",
    "for i in range(len(combined_names)):\n",
    "    print(i)\n",
    "    site_name = combined_names[i]\n",
    "    site_type = combined_types[i]\n",
    "    selected_ec = combined_ec[combined_ec[\"name\"] == site_name]\n",
    "\n",
    "    if selected_ec[\"PPFD_IN_QC\"].isna().all():\n",
    "        print(\"No PPFD data for \" + site_name)\n",
    "        bad_sites_list.append(site_name)\n",
    "        # continue\n",
    "    gpp = selected_ec[[\"GPP_NT_VUT_REF\"]]\n",
    "    par = selected_ec[[\"PPFD_IN\"]]\n",
    "    par_qc = selected_ec[[\"PPFD_IN_QC\"]]\n",
    "    ec_daily = pd.concat([gpp, par, par_qc], axis=1).rename(\n",
    "        columns={\"GPP_NT_VUT_REF\": \"gpp\", \"PPFD_IN\": \"par\", \"PPFD_IN_QC\": \"par_qc\"}\n",
    "    )\n",
    "    ec_daily = ec_daily[ec_daily[\"par_qc\"] == 1]\n",
    "    ec_daily = ec_daily[ec_daily[\"gpp\"] != -9999]\n",
    "\n",
    "    # filter fpar based on QC flags\n",
    "    filtered_fpar = selected_fpar[\n",
    "        (selected_fpar[\"MCD15A3H_061_FparLai_QC_MODLAND\"] == \"0b0\")\n",
    "        & (selected_fpar[\"MCD15A3H_061_FparLai_QC_DeadDetector\"] == \"0b0\")\n",
    "        & (selected_fpar[\"MCD15A3H_061_FparLai_QC_CloudState\"] == \"0b00\")\n",
    "        & (selected_fpar[\"MCD15A3H_061_FparLai_QC_SCF_QC\"].isin([\"0b000\", \"0b001\"]))\n",
    "    ].copy()\n",
    "\n",
    "    filtered_fpar.loc[:, \"time\"] = pd.to_datetime(filtered_fpar[\"Date\"])\n",
    "    fpar_4days = filtered_fpar[[\"MCD15A3H_061_Fpar_500m\"]]\n",
    "    fpar_4days.set_index(filtered_fpar[\"time\"], inplace=True)\n",
    "    fpar_daily = fpar_4days.resample(\"D\").interpolate(method=\"linear\")\n",
    "    fpar_daily.rename(columns={\"MCD15A3H_061_Fpar_500m\": \"fpar\"}, inplace=True)\n",
    "    if len(fpar_daily) < 30:\n",
    "        print(\"Not enough fpar for\" + site_name)\n",
    "        bad_sites_list.append(site_name)\n",
    "        continue\n",
    "    fpar_daily\n",
    "\n",
    "    # Now filter reflectance\n",
    "    filtered_refl = selected_refl[\n",
    "        (\n",
    "            selected_refl[\n",
    "                \"MCD43A4_061_BRDF_Albedo_Band_Mandatory_Quality_Band1_MODLAND\"\n",
    "            ]\n",
    "            == \"0b000\"\n",
    "        )\n",
    "        & (\n",
    "            selected_refl[\n",
    "                \"MCD43A4_061_BRDF_Albedo_Band_Mandatory_Quality_Band2_MODLAND\"\n",
    "            ]\n",
    "            == \"0b000\"\n",
    "        )\n",
    "    ].copy()\n",
    "    filtered_refl.loc[:, \"time\"] = pd.to_datetime(filtered_refl[\"Date\"])\n",
    "    red_daily = filtered_refl[[\"MCD43A4_061_Nadir_Reflectance_Band1\"]]\n",
    "    nir_daily = filtered_refl[[\"MCD43A4_061_Nadir_Reflectance_Band2\"]]\n",
    "\n",
    "    refl_daily = pd.concat([red_daily, nir_daily], axis=1).rename(\n",
    "        {\n",
    "            \"MCD43A4_061_Nadir_Reflectance_Band1\": \"red\",\n",
    "            \"MCD43A4_061_Nadir_Reflectance_Band2\": \"nir\",\n",
    "        },\n",
    "        axis=1,\n",
    "    )\n",
    "    refl_daily.set_index(filtered_refl[\"time\"], inplace=True)\n",
    "    daily_df = ec_daily.merge(refl_daily, left_index=True, right_index=True).merge(\n",
    "        fpar_daily, left_index=True, right_index=True\n",
    "    )\n",
    "    daily_df.loc[:, \"ndvi\"] = (daily_df[\"nir\"] - daily_df[\"red\"]) / (\n",
    "        daily_df[\"nir\"] + daily_df[\"red\"]\n",
    "    )\n",
    "    daily_df.loc[:, \"nirv\"] = daily_df[\"ndvi\"] * daily_df[\"nir\"]\n",
    "    daily_df.loc[:, \"nirvp\"] = daily_df[\"nirv\"] * daily_df[\"par\"]\n",
    "    daily_df.loc[:, \"fesc\"] = daily_df[\"nirv\"] / daily_df[\"fpar\"]\n",
    "    daily_df.loc[:, \"dasf\"] = daily_df[\"nirv\"] / 0.9789\n",
    "\n",
    "    daily_df.loc[:, \"lue\"] = daily_df[\"gpp\"] / (daily_df[\"par\"] * daily_df[\"fpar\"])\n",
    "    daily_df = daily_df.replace([-np.inf, np.inf], np.nan).dropna()\n",
    "\n",
    "    selected_ec_years = daily_df.index.year.unique()\n",
    "    if len(selected_ec_years) < 3:\n",
    "        print(\"Not enough years for \" + site_name)\n",
    "        bad_sites_list.append(site_name)\n",
    "        continue\n",
    "    # Assuming df is your DataFrame\n",
    "    outlier_rows_mask = remove_outliers(daily_df)\n",
    "    df_no_outliers = daily_df[~outlier_rows_mask].copy()\n",
    "    df_stat_pft.append(site_type)\n",
    "\n",
    "    # max_lue_index = df_no_outliers[\"lue\"].idxmax()\n",
    "    # df_stat.append(df_no_outliers.loc[max_lue_index])\n",
    "\n",
    "    median_lue = df_no_outliers[\"lue\"].median()\n",
    "    median_lue_index = (df_no_outliers[\"lue\"] - median_lue).abs().idxmin()\n",
    "    df_stat.append(df_no_outliers.loc[median_lue_index])\n",
    "    \n",
    "\n",
    "    # smoothed_lue = df_no_outliers.lue.rolling(window=7).mean()\n",
    "    # max_lue_index = smoothed_lue.groupby(smoothed_lue.index.year).idxmax()\n",
    "    # max_lue_index.dropna(inplace=True)\n",
    "    # df_stat.append(df_no_outliers.loc[max_lue_index].mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stat = pd.concat(df_stat, axis=1).T\n",
    "df_stat_pft = pd.DataFrame(df_stat_pft, index=df_stat.index)\n",
    "df_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_index = remove_outliers(df_stat)\n",
    "df_stat_clean = df_stat[~outlier_index].copy()\n",
    "df_stat_pft_clean = df_stat_pft[~outlier_index]\n",
    "df_stat_clean = df_stat_clean.assign(pft_clean=df_stat_pft_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do this later! \n",
    "# Define the bin edges\n",
    "# bin_edges = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "\n",
    "# Bin the data based on fpar\n",
    "# df_stat_clean[\"fpar_bin\"] = pd.cut(df_stat_clean[\"fpar\"], bins=bin_edges)\n",
    "# df_stat_clean\n",
    "# Select data in a specific bin\n",
    "# df_bin = df_stat_clean[df_stat_clean[\"fpar_bin\"] == pd.Interval(0.6, 0.8, closed=\"right\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df_melted = pd.melt(df_stat_clean, id_vars=\"pft_clean\", value_vars=[\"lue\", \"fesc\"])\n",
    "\n",
    "# Create a box plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=\"pft_clean\", y=\"value\", hue=\"variable\", data=df_melted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a FacetGrid with each unique value of pft_clean\n",
    "g = sns.FacetGrid(df_stat_clean, col=\"pft_clean\", col_wrap=3)\n",
    "\n",
    "# Map scatter plot to each subplot\n",
    "g.map_dataframe(sns.scatterplot, x=\"lue\", y=\"fesc\")\n",
    "\n",
    "# Set the labels and title\n",
    "g.set_axis_labels(\"lue\", \"fesc\")\n",
    "g.fig.suptitle(\"Scatter Plot: lue vs fesc for each pft_clean\", y=1.02)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Create a scatter plot with hue based on pft_clean\n",
    "sns.scatterplot(data=df_stat_clean, x=\"lue\", y=\"fesc\", hue=\"pft_clean\")\n",
    "\n",
    "# Set the labels and title\n",
    "plt.xlabel(\"lue\")\n",
    "plt.ylabel(\"fesc\")\n",
    "plt.title(\"Scatter Plot: lue vs fesc\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = [\n",
    "    \"gpp\",\n",
    "    \"lue\",\n",
    "    \"fesc\",\n",
    "    \"nirv\",\n",
    "    \"nirvp\",\n",
    "    \"dasf\",\n",
    "    \"ndvi\",\n",
    "    \"fpar\",\n",
    "    \"par\",\n",
    "    \"pft_clean\",\n",
    "]\n",
    "df_mean = df_stat_clean[var_list].groupby(\"pft_clean\").mean()\n",
    "df_mean = df_mean.reset_index()\n",
    "df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(9, 4))\n",
    "\n",
    "x = \"lue\"\n",
    "y = \"fpar\"\n",
    "\n",
    "scatter = sns.scatterplot(data=df_mean, x=x, y=y, hue=\"pft_clean\", ax=ax[0])\n",
    "for line in range(0, df_mean.shape[0]):\n",
    "    scatter.text(\n",
    "        df_mean[x][line] + 0.001,\n",
    "        df_mean[y][line],\n",
    "        df_mean.pft_clean[line],\n",
    "        horizontalalignment=\"left\",\n",
    "        size=\"medium\",\n",
    "        color=\"black\",\n",
    "        weight=\"semibold\",\n",
    "    )\n",
    "\n",
    "ax[0].set_xlabel(x)\n",
    "ax[0].set_ylabel(y)\n",
    "ax[0].set_title(x + \" vs \" + y)\n",
    "scatter.legend_.remove()\n",
    "x = \"gpp\"\n",
    "y = \"fesc\"\n",
    "\n",
    "scatter = sns.scatterplot(data=df_mean, x=x, y=y, hue=\"pft_clean\", ax=ax[1])\n",
    "for line in range(0, df_mean.shape[0]):\n",
    "    scatter.text(\n",
    "        df_mean[x][line] + 0.001,\n",
    "        df_mean[y][line],\n",
    "        df_mean.pft_clean[line],\n",
    "        horizontalalignment=\"left\",\n",
    "        size=\"medium\",\n",
    "        color=\"black\",\n",
    "        weight=\"semibold\",\n",
    "    )\n",
    "\n",
    "ax[1].set_xlabel(x)\n",
    "ax[1].set_ylabel(y)\n",
    "ax[1].set_title(x + \" vs \" + y)\n",
    "\n",
    "# Remove the legend\n",
    "scatter.legend_.remove()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean = df_mean.set_index(\"pft_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "axs = scatter_matrix(df_mean, figsize=(10, 10), alpha=1)\n",
    "# Loop over the diagonal\n",
    "for i in range(len(df_mean.columns)):\n",
    "    for j in range(len(df_mean.columns)):\n",
    "        if i != j:\n",
    "            # Calculate the R^2 value\n",
    "            r2 = (\n",
    "                np.corrcoef(df_mean[df_mean.columns[i]], df_mean[df_mean.columns[j]])[\n",
    "                    0, 1\n",
    "                ]\n",
    "                ** 2\n",
    "            )\n",
    "            # Add the R^2 value to the plot\n",
    "            axs[i, j].annotate(\n",
    "                \"R^2 = {:.2f}\".format(r2),\n",
    "                (0.5, 0.8),\n",
    "                xycoords=\"axes fraction\",\n",
    "                ha=\"center\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below code are for time series plot of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 335 # US-Ha1\n",
    "bad_sites = []\n",
    "\n",
    "i = 186\n",
    "site_name = combined_names[i]\n",
    "site_type = combined_types[i]\n",
    "print([site_name, site_type])\n",
    "selected_ec = combined_ec[combined_ec[\"name\"] == site_name]\n",
    "selected_refl = combined_refl[combined_refl[\"name\"] == site_name]\n",
    "selected_fpar = combined_fpar[combined_fpar[\"name\"] == site_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_ec[\"PPFD_IN_QC\"].isna().all():\n",
    "    print(\"No PPFD data for this site\")\n",
    "    bad_sites.append(site_name)\n",
    "    # continue\n",
    "\n",
    "gpp = selected_ec[[\"GPP_NT_VUT_REF\"]]\n",
    "par = selected_ec[[\"PPFD_IN\"]]\n",
    "par_qc = selected_ec[[\"PPFD_IN_QC\"]]\n",
    "ec_daily = pd.concat([gpp, par, par_qc], axis=1).rename(\n",
    "    columns={\"GPP_NT_VUT_REF\": \"gpp\", \"PPFD_IN\": \"par\", \"PPFD_IN_QC\": \"par_qc\"}\n",
    ")\n",
    "ec_daily = ec_daily[ec_daily[\"par_qc\"] == 1]\n",
    "ec_daily = ec_daily[ec_daily[\"gpp\"] != -9999]\n",
    "if len(ec_daily) < 30:\n",
    "    print(\"Not enough data for this site\")\n",
    "    bad_sites.append(site_name)\n",
    "    # continue\n",
    "\n",
    "ec_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter fpar based on QC flags\n",
    "filtered_fpar = selected_fpar[\n",
    "    (selected_fpar[\"MCD15A3H_061_FparLai_QC_MODLAND\"] == \"0b0\")\n",
    "    & (selected_fpar[\"MCD15A3H_061_FparLai_QC_DeadDetector\"] == \"0b0\")\n",
    "    & (selected_fpar[\"MCD15A3H_061_FparLai_QC_CloudState\"] == \"0b00\")\n",
    "    & (selected_fpar[\"MCD15A3H_061_FparLai_QC_SCF_QC\"].isin([\"0b000\", \"0b001\"]))\n",
    "].copy()\n",
    "\n",
    "filtered_fpar.loc[:, \"time\"] = pd.to_datetime(filtered_fpar[\"Date\"])\n",
    "fpar_4days = filtered_fpar[[\"MCD15A3H_061_Fpar_500m\"]]\n",
    "fpar_4days.set_index(filtered_fpar[\"time\"], inplace=True)\n",
    "fpar_daily = fpar_4days.resample(\"D\").interpolate(method=\"linear\")\n",
    "fpar_daily.rename(columns={\"MCD15A3H_061_Fpar_500m\": \"fpar\"}, inplace=True)\n",
    "if len(fpar_daily) < 30:\n",
    "    print(\"Not enough fpar for\" + site_name)\n",
    "    bad_sites.append(site_name)\n",
    "    # continue\n",
    "fpar_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now filter reflectance\n",
    "filtered_refl = selected_refl[\n",
    "    (\n",
    "        selected_refl[\"MCD43A4_061_BRDF_Albedo_Band_Mandatory_Quality_Band1_MODLAND\"]\n",
    "        == \"0b000\"\n",
    "    )\n",
    "    & (\n",
    "        selected_refl[\"MCD43A4_061_BRDF_Albedo_Band_Mandatory_Quality_Band2_MODLAND\"]\n",
    "        == \"0b000\"\n",
    "    )\n",
    "    & (\n",
    "        selected_refl[\"MCD43A4_061_BRDF_Albedo_Band_Mandatory_Quality_Band4_MODLAND\"]\n",
    "        == \"0b000\"\n",
    "    )\n",
    "].copy()\n",
    "filtered_refl.loc[:, \"time\"] = pd.to_datetime(filtered_refl[\"Date\"])\n",
    "red_daily = filtered_refl[[\"MCD43A4_061_Nadir_Reflectance_Band1\"]]\n",
    "nir_daily = filtered_refl[[\"MCD43A4_061_Nadir_Reflectance_Band2\"]]\n",
    "green_daily = filtered_refl[[\"MCD43A4_061_Nadir_Reflectance_Band4\"]]\n",
    "\n",
    "refl_daily = pd.concat([red_daily, nir_daily], axis=1).rename(\n",
    "    {\n",
    "        \"MCD43A4_061_Nadir_Reflectance_Band1\": \"red\",\n",
    "        \"MCD43A4_061_Nadir_Reflectance_Band2\": \"nir\",\n",
    "        \"MCD43A4_061_Nadir_Reflectance_Band4\": \"green\",\n",
    "    },\n",
    "    axis=1,\n",
    ")\n",
    "refl_daily.set_index(filtered_refl[\"time\"], inplace=True)\n",
    "refl_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df = ec_daily.merge(refl_daily, left_index=True, right_index=True).merge(\n",
    "    fpar_daily, left_index=True, right_index=True\n",
    ")\n",
    "daily_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df.loc[:, \"ndvi\"] = (daily_df[\"nir\"] - daily_df[\"red\"]) / (\n",
    "    daily_df[\"nir\"] + daily_df[\"red\"]\n",
    ")\n",
    "daily_df.loc[:, \"nirv\"] = daily_df[\"ndvi\"] * daily_df[\"nir\"]\n",
    "daily_df.loc[:, \"nirvp\"] = daily_df[\"nirv\"] * daily_df[\"par\"]\n",
    "daily_df.loc[:, \"fesc\"] = daily_df[\"nirv\"] / daily_df[\"fpar\"]\n",
    "\n",
    "daily_df.loc[:, \"lue\"] = daily_df[\"gpp\"] / (daily_df[\"par\"] * daily_df[\"fpar\"])\n",
    "daily_df = daily_df.replace([-np.inf, np.inf], np.nan).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Z-scores\n",
    "z_scores = np.abs(zscore(daily_df))\n",
    "\n",
    "# Set a threshold for outliers\n",
    "threshold = 3\n",
    "\n",
    "# Get a boolean mask where True indicates it is an outlier\n",
    "outliers = (z_scores > threshold).any(axis=1)\n",
    "\n",
    "# Remove outliers\n",
    "daily_df_no_outliers = daily_df[~outliers].copy()\n",
    "daily_df_no_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [\"lue\", \"fesc\", \"nirv\", \"nirvp\", \"ndvi\", \"gpp\"]\n",
    "scatter_matrix(daily_df_no_outliers[attributes], figsize=(10, 10))\n",
    "r2_values_daily = {}\n",
    "attributes = [\"lue\", \"fesc\", \"nirv\", \"nirvp\", \"ndvi\", \"gpp\"]\n",
    "\n",
    "for x in attributes:\n",
    "    for y in attributes:\n",
    "        if x != y:\n",
    "            slope, intercept, r_value, p_value, std_err = linregress(\n",
    "                daily_df_no_outliers[x], daily_df_no_outliers[y]\n",
    "            )\n",
    "            r2_values_daily[(x, y)] = r_value**2\n",
    "\n",
    "r2_values_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sites = []\n",
    "\n",
    "i = 2\n",
    "site_name = combined_names[i]\n",
    "site_type = combined_types[i]\n",
    "print([site_name, site_type])\n",
    "selected_ec = combined_ec[combined_ec[\"name\"] == site_name]\n",
    "selected_refl = combined_refl[combined_refl[\"name\"] == site_name]\n",
    "selected_fpar = combined_fpar[combined_fpar[\"name\"] == site_name]\n",
    "\n",
    "if selected_ec[\"PPFD_IN_QC\"].isna().all():\n",
    "    print(\"No PPFD data for this site\")\n",
    "    bad_sites.append(site_name)\n",
    "    # continue\n",
    "\n",
    "gpp = selected_ec[[\"GPP_NT_VUT_REF\"]]\n",
    "par = selected_ec[[\"PPFD_IN\"]]\n",
    "par_qc = selected_ec[[\"PPFD_IN_QC\"]]\n",
    "ec_daily = pd.concat([gpp, par, par_qc], axis=1).rename(\n",
    "    columns={\"GPP_NT_VUT_REF\": \"gpp\", \"PPFD_IN\": \"par\", \"PPFD_IN_QC\": \"par_qc\"}\n",
    ")\n",
    "ec_daily = ec_daily[ec_daily[\"par_qc\"] == 1]\n",
    "ec_daily = ec_daily[ec_daily[\"gpp\"] != -9999]\n",
    "if len(ec_daily) < 30:\n",
    "    print(\"Not enough data for this site\")\n",
    "    bad_sites.append(site_name)\n",
    "    # continue\n",
    "\n",
    "filtered_fpar = selected_fpar[\n",
    "    (selected_fpar[\"MCD15A3H_061_FparLai_QC_MODLAND\"] == \"0b0\")\n",
    "    & (selected_fpar[\"MCD15A3H_061_FparLai_QC_DeadDetector\"] == \"0b0\")\n",
    "    & (selected_fpar[\"MCD15A3H_061_FparLai_QC_CloudState\"] == \"0b00\")\n",
    "    & (selected_fpar[\"MCD15A3H_061_FparLai_QC_SCF_QC\"].isin([\"0b000\", \"0b001\"]))\n",
    "].copy()\n",
    "\n",
    "filtered_fpar.loc[:, \"time\"] = pd.to_datetime(filtered_fpar[\"Date\"])\n",
    "fpar_4days = filtered_fpar[[\"MCD15A3H_061_Fpar_500m\"]]\n",
    "fpar_4days.set_index(filtered_fpar[\"time\"], inplace=True)\n",
    "fpar_daily = fpar_4days.resample(\"D\").interpolate(method=\"linear\")\n",
    "fpar_daily.rename(columns={\"MCD15A3H_061_Fpar_500m\": \"fpar\"}, inplace=True)\n",
    "\n",
    "filtered_refl = selected_refl[\n",
    "    (\n",
    "        selected_refl[\"MCD43A4_061_BRDF_Albedo_Band_Mandatory_Quality_Band1_MODLAND\"]\n",
    "        == \"0b000\"\n",
    "    )\n",
    "    & (\n",
    "        selected_refl[\"MCD43A4_061_BRDF_Albedo_Band_Mandatory_Quality_Band2_MODLAND\"]\n",
    "        == \"0b000\"\n",
    "    )\n",
    "].copy()\n",
    "filtered_refl.loc[:, \"time\"] = pd.to_datetime(filtered_refl[\"Date\"])\n",
    "red_daily = filtered_refl[[\"MCD43A4_061_Nadir_Reflectance_Band1\"]]\n",
    "nir_daily = filtered_refl[[\"MCD43A4_061_Nadir_Reflectance_Band2\"]]\n",
    "\n",
    "refl_daily = pd.concat([red_daily, nir_daily], axis=1).rename(\n",
    "    {\n",
    "        \"MCD43A4_061_Nadir_Reflectance_Band1\": \"red\",\n",
    "        \"MCD43A4_061_Nadir_Reflectance_Band2\": \"nir\",\n",
    "    },\n",
    "    axis=1,\n",
    ")\n",
    "refl_daily.set_index(filtered_refl[\"time\"], inplace=True)\n",
    "daily_df = ec_daily.merge(refl_daily, left_index=True, right_index=True).merge(\n",
    "    fpar_daily, left_index=True, right_index=True\n",
    ")\n",
    "\n",
    "daily_df.loc[:, \"ndvi\"] = (daily_df[\"nir\"] - daily_df[\"red\"]) / (\n",
    "    daily_df[\"nir\"] + daily_df[\"red\"]\n",
    ")\n",
    "daily_df.loc[:, \"nirv\"] = daily_df[\"ndvi\"] * daily_df[\"nir\"]\n",
    "daily_df.loc[:, \"nirvp\"] = daily_df[\"nirv\"] * daily_df[\"par\"]\n",
    "daily_df.loc[:, \"fesc\"] = daily_df[\"nirv\"] / daily_df[\"fpar\"]\n",
    "\n",
    "daily_df.loc[:, \"lue\"] = daily_df[\"gpp\"] / (daily_df[\"par\"] * daily_df[\"fpar\"])\n",
    "attributes = [\"lue\", \"fesc\", \"nirv\", \"nirvp\", \"ndvi\", \"gpp\"]\n",
    "scatter_matrix(daily_df_no_outliers[attributes], figsize=(10, 10))\n",
    "r2_values_daily = {}\n",
    "attributes = [\"lue\", \"fesc\", \"nirv\", \"nirvp\", \"ndvi\", \"gpp\"]\n",
    "\n",
    "for x in attributes:\n",
    "    for y in attributes:\n",
    "        if x != y:\n",
    "            slope, intercept, r_value, p_value, std_err = linregress(\n",
    "                daily_df_no_outliers[x], daily_df_no_outliers[y]\n",
    "            )\n",
    "            r2_values_daily[(x, y)] = r_value**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "from pandas.plotting import scatter_matrix\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def process_site_data(\n",
    "    i, combined_names, combined_types, combined_ec, combined_refl, combined_fpar\n",
    "):\n",
    "    bad_sites = []\n",
    "    site_name = combined_names[i]\n",
    "    site_type = combined_types[i]\n",
    "    selected_ec = combined_ec[combined_ec[\"name\"] == site_name]\n",
    "    selected_refl = combined_refl[combined_refl[\"name\"] == site_name]\n",
    "    selected_fpar = combined_fpar[combined_fpar[\"name\"] == site_name]\n",
    "\n",
    "    if selected_ec[\"PPFD_IN_QC\"].isna().all():\n",
    "        print(\"No PPFD data for \" + site_name)\n",
    "        bad_sites.append(site_name)\n",
    "        return (\n",
    "            bad_sites,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "        )\n",
    "\n",
    "    ec_daily = process_ec_data(selected_ec)\n",
    "    if len(ec_daily) < 30:\n",
    "        print(\"Not enough data for \" + site_name)\n",
    "        bad_sites.append(site_name)\n",
    "        return (\n",
    "            bad_sites,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "        )\n",
    "\n",
    "    fpar_daily = process_fpar_data(selected_fpar)\n",
    "    if len(fpar_daily) < 30:\n",
    "        print(\"Not enough fpar for\" + site_name)\n",
    "        bad_sites.append(site_name)\n",
    "        return (\n",
    "            bad_sites,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "        )\n",
    "\n",
    "    refl_daily = process_refl_data(selected_refl)\n",
    "    if len(refl_daily) < 30:\n",
    "        print(\"Not enough reflectance data for \" + site_name)\n",
    "        bad_sites.append(site_name)\n",
    "        return (\n",
    "            bad_sites,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "        )\n",
    "\n",
    "    daily_df = merge_data(ec_daily, refl_daily, fpar_daily)\n",
    "    if len(daily_df) < 30:\n",
    "        print(\"Not enough data for \" + site_name)\n",
    "        bad_sites.append(site_name)\n",
    "        return (\n",
    "            bad_sites,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "            None,\n",
    "        )\n",
    "    daily_df = calculate_indices(daily_df)\n",
    "    daily_df = daily_df.replace([-np.inf, np.inf], np.nan).dropna()\n",
    "    daily_df_no_outliers = remove_outliers(\n",
    "        daily_df\n",
    "    )  # Assuming you have a way to remove outliers\n",
    "\n",
    "    r2_values_daily, p_value_daily = calculate_r2_values(daily_df_no_outliers)\n",
    "\n",
    "    weekly_df = daily_df_no_outliers.resample(\"W\").mean()\n",
    "    weekly_df_no_outliers = remove_outliers(weekly_df)\n",
    "    weekly_df_no_outliers.dropna(inplace=True)\n",
    "    r2_values_weekly, p_value_weekly = calculate_r2_values(weekly_df_no_outliers)\n",
    "\n",
    "    monthly_df = daily_df_no_outliers.resample(\"M\").mean()\n",
    "    monthly_df_no_outliers = remove_outliers(monthly_df)\n",
    "    monthly_df_no_outliers.dropna(inplace=True)\n",
    "    r2_values_monthly, p_value_monthly = calculate_r2_values(monthly_df_no_outliers)\n",
    "\n",
    "    return (\n",
    "        bad_sites,\n",
    "        r2_values_daily,\n",
    "        p_value_daily,\n",
    "        r2_values_weekly,\n",
    "        p_value_weekly,\n",
    "        r2_values_monthly,\n",
    "        p_value_monthly,\n",
    "        daily_df_no_outliers,\n",
    "        weekly_df_no_outliers,\n",
    "        monthly_df_no_outliers,\n",
    "        site_type,\n",
    "        site_name,\n",
    "    )\n",
    "\n",
    "\n",
    "def process_ec_data(selected_ec):\n",
    "    gpp = selected_ec[[\"GPP_NT_VUT_REF\"]]\n",
    "    par = selected_ec[[\"PPFD_IN\"]]\n",
    "    par_qc = selected_ec[[\"PPFD_IN_QC\"]]\n",
    "    ec_daily = pd.concat([gpp, par, par_qc], axis=1).rename(\n",
    "        columns={\"GPP_NT_VUT_REF\": \"gpp\", \"PPFD_IN\": \"par\", \"PPFD_IN_QC\": \"par_qc\"}\n",
    "    )\n",
    "    ec_daily = ec_daily[ec_daily[\"par_qc\"] == 1]\n",
    "    ec_daily = ec_daily[ec_daily[\"gpp\"] != -9999]\n",
    "    return ec_daily\n",
    "\n",
    "\n",
    "def process_fpar_data(selected_fpar):\n",
    "    # Your code here\n",
    "    # filter fpar based on QC flags\n",
    "    filtered_fpar = selected_fpar[\n",
    "        (selected_fpar[\"MCD15A3H_061_FparLai_QC_MODLAND\"] == \"0b0\")\n",
    "        & (selected_fpar[\"MCD15A3H_061_FparLai_QC_DeadDetector\"] == \"0b0\")\n",
    "        & (selected_fpar[\"MCD15A3H_061_FparLai_QC_CloudState\"] == \"0b00\")\n",
    "        & (selected_fpar[\"MCD15A3H_061_FparLai_QC_SCF_QC\"].isin([\"0b000\", \"0b001\"]))\n",
    "    ].copy()\n",
    "\n",
    "    filtered_fpar.loc[:, \"time\"] = pd.to_datetime(filtered_fpar[\"Date\"])\n",
    "    fpar_4days = filtered_fpar[[\"MCD15A3H_061_Fpar_500m\"]]\n",
    "    fpar_4days.set_index(filtered_fpar[\"time\"], inplace=True)\n",
    "    fpar_daily = fpar_4days.resample(\"D\").interpolate(method=\"linear\")\n",
    "    fpar_daily.rename(columns={\"MCD15A3H_061_Fpar_500m\": \"fpar\"}, inplace=True)\n",
    "    return fpar_daily\n",
    "\n",
    "\n",
    "def process_refl_data(selected_refl):\n",
    "    # Your code here\n",
    "    filtered_refl = selected_refl[\n",
    "        (\n",
    "            selected_refl[\n",
    "                \"MCD43A4_061_BRDF_Albedo_Band_Mandatory_Quality_Band1_MODLAND\"\n",
    "            ]\n",
    "            == \"0b000\"\n",
    "        )\n",
    "        & (\n",
    "            selected_refl[\n",
    "                \"MCD43A4_061_BRDF_Albedo_Band_Mandatory_Quality_Band2_MODLAND\"\n",
    "            ]\n",
    "            == \"0b000\"\n",
    "        )\n",
    "    ].copy()\n",
    "    filtered_refl.loc[:, \"time\"] = pd.to_datetime(filtered_refl[\"Date\"])\n",
    "    red_daily = filtered_refl[[\"MCD43A4_061_Nadir_Reflectance_Band1\"]]\n",
    "    nir_daily = filtered_refl[[\"MCD43A4_061_Nadir_Reflectance_Band2\"]]\n",
    "\n",
    "    refl_daily = pd.concat([red_daily, nir_daily], axis=1).rename(\n",
    "        {\n",
    "            \"MCD43A4_061_Nadir_Reflectance_Band1\": \"red\",\n",
    "            \"MCD43A4_061_Nadir_Reflectance_Band2\": \"nir\",\n",
    "        },\n",
    "        axis=1,\n",
    "    )\n",
    "    refl_daily.set_index(filtered_refl[\"time\"], inplace=True)\n",
    "    return refl_daily\n",
    "\n",
    "\n",
    "def merge_data(ec_daily, refl_daily, fpar_daily):\n",
    "    # Your code here\n",
    "    daily_df = ec_daily.merge(refl_daily, left_index=True, right_index=True).merge(\n",
    "        fpar_daily, left_index=True, right_index=True\n",
    "    )\n",
    "    return daily_df\n",
    "\n",
    "\n",
    "def calculate_indices(daily_df):\n",
    "    # w_nir = 0.9789\n",
    "    # w_green = 0.4898\n",
    "    daily_df.loc[:, \"ndvi\"] = (daily_df[\"nir\"] - daily_df[\"red\"]) / (\n",
    "        daily_df[\"nir\"] + daily_df[\"red\"]\n",
    "    )\n",
    "    daily_df.loc[:, \"nirv\"] = daily_df[\"ndvi\"] * daily_df[\"nir\"]\n",
    "    daily_df.loc[:, \"nirvp\"] = daily_df[\"nirv\"] * daily_df[\"par\"]\n",
    "    daily_df.loc[:, \"fesc\"] = daily_df[\"nirv\"] / daily_df[\"fpar\"]\n",
    "    # daily_df.loc[:, \"p\"] = (\n",
    "    #     (daily_df[\"nir\"] / w_nir) - (daily_df[\"green\"] / w_green)\n",
    "    # ) / (daily_df[\"nir\"] - daily_df[\"green\"])\n",
    "    daily_df.loc[:, \"lue\"] = daily_df[\"gpp\"] / (daily_df[\"par\"] * daily_df[\"fpar\"])\n",
    "    return daily_df\n",
    "\n",
    "\n",
    "def remove_outliers(df):\n",
    "    # Your code here\n",
    "    z_scores = np.abs(zscore(df))\n",
    "\n",
    "    # Set a threshold for outliers\n",
    "    threshold = 2\n",
    "\n",
    "    # Get a boolean mask where True indicates it is an outlier\n",
    "    outliers = (z_scores > threshold).any(axis=1)\n",
    "\n",
    "    # Remove outliers\n",
    "    df_no_outliers = df[~outliers].copy()\n",
    "    return df_no_outliers\n",
    "\n",
    "\n",
    "def calculate_r2_values(df):\n",
    "    r2_values = {}\n",
    "    p_values = {}\n",
    "    attributes = [\"lue\", \"fesc\", \"nirv\", \"nirvp\", \"ndvi\", \"gpp\"]\n",
    "    for x in attributes:\n",
    "        for y in attributes:\n",
    "            if x != y:\n",
    "                slope, intercept, r_value, p_value, std_err = linregress(df[x], df[y])\n",
    "                r2_values[(x, y)] = r_value**2\n",
    "                p_values[(x, y)] = p_value\n",
    "    return (r2_values, p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 106  # US-Ha1\n",
    "# i = 186\n",
    "(\n",
    "    bad_sites,\n",
    "    r2_values_daily,\n",
    "    p_value_daily,\n",
    "    r2_values_weekly,\n",
    "    p_value_weekly,\n",
    "    r2_values_monthly,\n",
    "    p_value_monthly,\n",
    "    daily_df_no_outliers,\n",
    "    weekly_df_no_outliers,\n",
    "    monthly_df_no_outliers,\n",
    "    site_type,\n",
    "    site_name,\n",
    ") = process_site_data(\n",
    "    i, combined_names, combined_types, combined_ec, combined_refl, combined_fpar\n",
    ")\n",
    "site_name, r2_values_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sites_list = []\n",
    "good_sites_data = []\n",
    "\n",
    "# Loop through sites\n",
    "for i in range(len(combined_names)):\n",
    "    print(i)\n",
    "    (\n",
    "        bad_sites,\n",
    "        r2_values_daily,\n",
    "        p_value_daily,\n",
    "        r2_values_weekly,\n",
    "        p_value_weekly,\n",
    "        r2_values_monthly,\n",
    "        p_value_monthly,\n",
    "        daily_df_no_outliers,\n",
    "        weekly_df_no_outliers,\n",
    "        monthly_df_no_outliers,\n",
    "        site_type,\n",
    "        site_name,\n",
    "    ) = process_site_data(\n",
    "        i, combined_names, combined_types, combined_ec, combined_refl, combined_fpar\n",
    "    )\n",
    "\n",
    "    if len(bad_sites) > 0:\n",
    "        bad_sites_list.extend(bad_sites)\n",
    "    else:\n",
    "        good_sites_data.append(\n",
    "            {\n",
    "                \"Site Name\": site_name,\n",
    "                \"Site Type\": site_type,\n",
    "                \"R2 Values daily\": r2_values_daily,\n",
    "                \"p Values daily\": p_value_daily,\n",
    "                \"R2 Values weekly\": r2_values_weekly,\n",
    "                \"p Values weekly\": p_value_weekly,\n",
    "                \"R2 Values monthly\": r2_values_monthly,\n",
    "                \"p Values monthly\": p_value_monthly,\n",
    "                \"daily_df_no_outliers\": daily_df_no_outliers,\n",
    "                \"weekly_df_no_outliers\": weekly_df_no_outliers,\n",
    "                \"monthly_df_no_outliers\": monthly_df_no_outliers,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Create a DataFrame from good_sites_data\n",
    "df = pd.DataFrame(good_sites_data)\n",
    "\n",
    "# Extract R2 values into a separate DataFrame\n",
    "r2_df = pd.DataFrame(df[\"R2 Values weekly\"].tolist())\n",
    "r2_df[\"Site Name\"] = df[\"Site Name\"]\n",
    "r2_df[\"Site Type\"] = df[\"Site Type\"]\n",
    "# Melt the DataFrame to a long format\n",
    "melted_df = r2_df.melt(\n",
    "    id_vars=[\"Site Name\", \"Site Type\"], var_name=\"Pair\", value_name=\"R2\"\n",
    ")\n",
    "selected_pairs = [(\"lue\", \"fesc\"), (\"lue\", \"nirv\"), (\"lue\", \"nirvp\")]\n",
    "filtered_df = melted_df[melted_df[\"Pair\"].isin(selected_pairs)]\n",
    "sns.boxplot(data=filtered_df, x=\"Site Type\", y=\"R2\", hue=\"Pair\")\n",
    "plt.savefig(\"../outputs/all_sites_weekly_r2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a DataFrame from good_sites_data\n",
    "df = pd.DataFrame(good_sites_data)\n",
    "\n",
    "# Extract R2 values into a separate DataFrame\n",
    "r2_df = pd.DataFrame(df[\"R2 Values weekly\"].tolist())\n",
    "r2_df[\"Site Name\"] = df[\"Site Name\"]\n",
    "r2_df[\"Site Type\"] = df[\"Site Type\"]\n",
    "\n",
    "# Melt the DataFrame to a long format\n",
    "melted_df = r2_df.melt(\n",
    "    id_vars=[\"Site Name\", \"Site Type\"], var_name=\"Pair\", value_name=\"R2\"\n",
    ")\n",
    "selected_pairs = [(\"lue\", \"fesc\"), (\"lue\", \"nirv\"), (\"lue\", \"nirvp\")]\n",
    "filtered_df = melted_df[melted_df[\"Pair\"].isin(selected_pairs)]\n",
    "\n",
    "# Create boxplot\n",
    "ax = sns.boxplot(data=filtered_df, x=\"Site Type\", y=\"R2\", hue=\"Pair\")\n",
    "\n",
    "# Annotate number of samples in each boxplot\n",
    "for i, site_type in enumerate(filtered_df[\"Site Type\"].unique()):\n",
    "    # Select one of the pairs, for example \"lue\", \"fesc\"\n",
    "    pair = selected_pairs[0]\n",
    "    num_samples = len(\n",
    "        filtered_df[\n",
    "            (filtered_df[\"Site Type\"] == site_type) & (filtered_df[\"Pair\"] == pair)\n",
    "        ]\n",
    "    )\n",
    "    max_y = filtered_df[\n",
    "        (filtered_df[\"Site Type\"] == site_type) & (filtered_df[\"Pair\"] == pair)\n",
    "    ][\"R2\"].max()\n",
    "    ax.text(\n",
    "        i,\n",
    "        max_y + 0.01,\n",
    "        f\"n={num_samples}\",\n",
    "        horizontalalignment=\"center\",\n",
    "        size=\"small\",\n",
    "        color=\"black\",\n",
    "        weight=\"semibold\",\n",
    "    )\n",
    "\n",
    "plt.savefig(\"../outputs/all_sites_weekly_r2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all 'daily_df_no_outliers' DataFrames into one\n",
    "all_daily_df = pd.concat(\n",
    "    [\n",
    "        df.loc[i, \"daily_df_no_outliers\"].assign(site_type=df.loc[i, \"Site Type\"])\n",
    "        for i in df.index\n",
    "    ]\n",
    ")\n",
    "print(\"Number of daily data points:\", len(all_daily_df))\n",
    "print(\"Number of sites:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "all_daily_df_no_outlier = all_daily_df[\n",
    "    (np.abs(zscore(all_daily_df[\"lue\"])) < 2) | (all_daily_df[\"lue\"].isna())\n",
    "]\n",
    "all_daily_df_no_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.boxplot(data=all_daily_df_no_outlier, x=\"site_type\", y=\"fesc\")\n",
    "\n",
    "# Annotate number of samples in each boxplot\n",
    "for i, site_type in enumerate(all_daily_df_no_outlier[\"site_type\"].unique()):\n",
    "    num_samples = len(\n",
    "        all_daily_df_no_outlier[all_daily_df_no_outlier[\"site_type\"] == site_type]\n",
    "    )\n",
    "    max_y = all_daily_df_no_outlier[all_daily_df_no_outlier[\"site_type\"] == site_type][\n",
    "        \"lue\"\n",
    "    ].max()\n",
    "    ax.text(\n",
    "        i,\n",
    "        max_y - 0.01,\n",
    "        f\"n={num_samples}\",\n",
    "        horizontalalignment=\"center\",\n",
    "        size=\"small\",\n",
    "        color=\"black\",\n",
    "        weight=\"semibold\",\n",
    "    )\n",
    "\n",
    "plt.title(\"Boxplot of LUE for each Site Type\")\n",
    "plt.savefig(\"../outputs/lue_boxplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.boxplot(data=all_daily_df_no_outlier, x=\"site_type\", y=\"fpar\")\n",
    "\n",
    "# # Annotate number of samples in each boxplot\n",
    "# for i, site_type in enumerate(all_daily_df_no_outlier[\"site_type\"].unique()):\n",
    "#     num_samples = len(all_daily_df_no_outlier[all_daily_df_no_outlier[\"site_type\"] == site_type])\n",
    "#     max_y = all_daily_df_no_outlier[all_daily_df_no_outlier[\"site_type\"] == site_type][\"fpar\"].max()\n",
    "#     ax.text(i, max_y - 0.01, f'n={num_samples}', horizontalalignment='center', size='small', color='black', weight='semibold')\n",
    "\n",
    "plt.title(\"Boxplot of fpar for each Site Type\")\n",
    "plt.savefig(\"../outputs/fpar_boxplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries mapping site names to latitude and longitude\n",
    "ameriflux_dict = ameriflux_info.set_index(\"Site ID\")[\n",
    "    [\"Latitude (degrees)\", \"Longitude (degrees)\"]\n",
    "].to_dict(orient=\"index\")\n",
    "fluxnet_dict = fluxnet_info.set_index(\"ID\")[[\"lat\", \"lon\"]].to_dict(orient=\"index\")\n",
    "\n",
    "# Initialize an empty list to store the latitude and longitude values\n",
    "lat_lon_list = []\n",
    "\n",
    "# Loop through each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Extract site name from the row\n",
    "    site_name = row[\"Site Name\"]\n",
    "\n",
    "    # Get latitude and longitude from ameriflux_dict if it exists, otherwise get it from fluxnet_dict\n",
    "    if site_name in ameriflux_dict:\n",
    "        lat = ameriflux_dict[site_name][\"Latitude (degrees)\"]\n",
    "        lon = ameriflux_dict[site_name][\"Longitude (degrees)\"]\n",
    "    elif site_name in fluxnet_dict:\n",
    "        lat = fluxnet_dict[site_name][\"lat\"]\n",
    "        lon = fluxnet_dict[site_name][\"lon\"]\n",
    "    else:\n",
    "        lat = None\n",
    "        lon = None\n",
    "\n",
    "    # Append the latitude and longitude to the list\n",
    "    lat_lon_list.append({\"Site Name\": site_name, \"Latitude\": lat, \"Longitude\": lon})\n",
    "\n",
    "# Create a DataFrame from the list of latitude and longitude values\n",
    "lat_lon_df = pd.DataFrame(lat_lon_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Convert DataFrame to GeoDataFrame\n",
    "geometry = [Point(xy) for xy in zip(lat_lon_df[\"Longitude\"], lat_lon_df[\"Latitude\"])]\n",
    "geo_df = gpd.GeoDataFrame(lat_lon_df, geometry=geometry)\n",
    "\n",
    "# Set the GeoDataFrame's coordinate system to geographic (EPSG:4326)\n",
    "geo_df.set_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "# Save the GeoDataFrame as a GeoJSON file\n",
    "geo_df.to_file(\"../outputs/lat_lon.geojson\", driver=\"GeoJSON\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dscovr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
