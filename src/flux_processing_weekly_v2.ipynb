{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from scipy.stats import zscore, linregress\n",
    "from pandas.plotting import scatter_matrix\n",
    "from scipy import stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove outliers from a dataframe\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "\n",
    "def remove_outliers(df):\n",
    "    # Initialize a boolean mask to keep track of rows to drop\n",
    "    outlier_rows_mask = np.zeros(len(df), dtype=bool)\n",
    "\n",
    "    # Iterate over each column\n",
    "    for col in df.columns:\n",
    "        # Fit the LocalOutlierFactor model to the column data\n",
    "        lof = LocalOutlierFactor()\n",
    "        outliers = lof.fit_predict(df[col].values.reshape(-1, 1))\n",
    "\n",
    "        # Mark rows with outliers in this column\n",
    "        outlier_rows_mask = np.logical_or(outlier_rows_mask, outliers == -1)\n",
    "\n",
    "    # Drop rows with outliers\n",
    "    # cleaned_df = df[~outlier_rows_mask]\n",
    "    return outlier_rows_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluxnet_info = pd.read_csv(\"../data/EC/fluxnet/sites_info.csv\")\n",
    "ameriflux_info = pd.read_csv(\"../data/EC/Ameriflux/sites_info.tsv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluxnet_names = fluxnet_info[\"ID\"].to_list()\n",
    "fluxnet_types = fluxnet_info[\"type\"].to_list()\n",
    "ameriflux_names = ameriflux_info[\"Site ID\"].to_list()\n",
    "ameriflux_types = ameriflux_info[\"Vegetation Abbreviation (IGBP)\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_names = list(set(ameriflux_names + fluxnet_names))\n",
    "combined_types = []\n",
    "for name in combined_names:\n",
    "    if name in ameriflux_names and name in fluxnet_names:\n",
    "        # Choose a type from either fluxnet_types or ameriflux_types\n",
    "        combined_types.append(fluxnet_types[fluxnet_names.index(name)])\n",
    "    elif name in ameriflux_names:\n",
    "        combined_types.append(ameriflux_types[ameriflux_names.index(name)])\n",
    "    else:\n",
    "        combined_types.append(fluxnet_types[fluxnet_names.index(name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "# Iterate over the range of length of 'name' column in sites_info\n",
    "for i in range(len(combined_names)):\n",
    "    site_name = combined_names[i]\n",
    "    site_type = combined_types[i]\n",
    "\n",
    "    # Set the file path based on whether the combined name is in ameriflux_names or fluxnet_names\n",
    "    if site_name in ameriflux_names:\n",
    "        file = glob.glob(\"../data/EC/Ameriflux/AMF_\" + site_name + \"*WW*\")\n",
    "    else:\n",
    "        file = glob.glob(\"../data/EC/fluxnet/FLX_\" + site_name + \"*WW*\")\n",
    "\n",
    "    # Open the CSV file\n",
    "    ec = pd.read_csv(file[0])\n",
    "    ec.loc[:, \"type\"] = site_type\n",
    "    ec.loc[:, \"name\"] = site_name\n",
    "\n",
    "    ec[\"t1\"] = pd.to_datetime(ec[\"TIMESTAMP_START\"], format=\"%Y%m%d\")\n",
    "    ec[\"t2\"] = pd.to_datetime(ec[\"TIMESTAMP_END\"], format=\"%Y%m%d\")\n",
    "\n",
    "    # Append the DataFrame to the list\n",
    "    dfs.append(ec)\n",
    "\n",
    "# Concatenate all DataFrames in the list\n",
    "combined_ec = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MCD43_fluxnet = []\n",
    "MCD15_fluxnet = []\n",
    "MCD43_ameriflux = []\n",
    "MCD15_ameriflux = []\n",
    "\n",
    "# Loop over batches (#5) of downloaded data\n",
    "for i in range(1, 5):\n",
    "    refl_fluxnet = glob.glob(\n",
    "        \"../data/EC/fluxnet/sat_data/*batch\" + str(i) + \"*MCD43A4-061-results.csv\"\n",
    "    )\n",
    "    sat_refl_fluxnet = pd.read_csv(refl_fluxnet[0])\n",
    "    sat_refl_fluxnet.loc[:, \"time\"] = pd.to_datetime(sat_refl_fluxnet[\"Date\"])\n",
    "    sat_refl_fluxnet.set_index(sat_refl_fluxnet[\"Date\"], inplace=True)\n",
    "    MCD43_fluxnet.append(sat_refl_fluxnet)\n",
    "\n",
    "    fpar_fluxnet = glob.glob(\n",
    "        \"../data/EC/fluxnet/sat_data/*batch\" + str(i) + \"*MCD15A3H-061-results.csv\"\n",
    "    )\n",
    "    sat_fpar_fluxnet = pd.read_csv(fpar_fluxnet[0])\n",
    "    sat_fpar_fluxnet.loc[:, \"time\"] = pd.to_datetime(sat_fpar_fluxnet[\"Date\"])\n",
    "    sat_fpar_fluxnet.set_index(sat_fpar_fluxnet[\"Date\"], inplace=True)\n",
    "    MCD15_fluxnet.append(sat_fpar_fluxnet)\n",
    "\n",
    "    if i < 5:\n",
    "        refl_ameriflux = glob.glob(\n",
    "            \"../data/EC/Ameriflux/sat_data/*batch\" + str(i) + \"*MCD43A4-061-results.csv\"\n",
    "        )\n",
    "\n",
    "        sat_refl_ameriflux = pd.read_csv(refl_ameriflux[0])\n",
    "        sat_refl_ameriflux.loc[:, \"time\"] = pd.to_datetime(sat_refl_ameriflux[\"Date\"])\n",
    "        sat_refl_ameriflux.set_index(sat_refl_ameriflux[\"Date\"], inplace=True)\n",
    "        MCD43_ameriflux.append(sat_refl_ameriflux)\n",
    "\n",
    "        fpar_ameriflux = glob.glob(\n",
    "            \"../data/EC/Ameriflux/sat_data/*batch\"\n",
    "            + str(i)\n",
    "            + \"*MCD15A3H-061-results.csv\"\n",
    "        )\n",
    "        fpar_ameriflux = pd.read_csv(fpar_ameriflux[0])\n",
    "\n",
    "        fpar_ameriflux.loc[:, \"time\"] = pd.to_datetime(fpar_ameriflux[\"Date\"])\n",
    "        fpar_ameriflux.set_index(fpar_ameriflux[\"Date\"], inplace=True)\n",
    "        MCD15_ameriflux.append(fpar_ameriflux)\n",
    "\n",
    "\n",
    "refl_fluxnet = pd.concat(MCD43_fluxnet)\n",
    "refl_fluxnet = refl_fluxnet.rename(columns={\"ID\": \"name\"})\n",
    "\n",
    "fpar_fluxnet = pd.concat(MCD15_fluxnet)\n",
    "fpar_fluxnet = fpar_fluxnet.rename(columns={\"ID\": \"name\"})\n",
    "\n",
    "refl_ameriflux = pd.concat(MCD43_ameriflux)\n",
    "refl_ameriflux = refl_ameriflux.rename(columns={\"ID\": \"name\"})\n",
    "fpar_ameriflux = pd.concat(MCD15_ameriflux)\n",
    "fpar_ameriflux = fpar_ameriflux.rename(columns={\"ID\": \"name\"})\n",
    "combined_refl = []\n",
    "combined_fpar = []\n",
    "\n",
    "for name in combined_names:\n",
    "    if name in ameriflux_names:\n",
    "        selected_refl = refl_ameriflux[refl_ameriflux[\"name\"] == name]\n",
    "        selected_fpar = fpar_ameriflux[fpar_ameriflux[\"name\"] == name]\n",
    "    else:\n",
    "        selected_refl = refl_fluxnet[refl_fluxnet[\"name\"] == name]\n",
    "        selected_fpar = fpar_fluxnet[fpar_fluxnet[\"name\"] == name]\n",
    "\n",
    "    combined_refl.append(selected_refl)\n",
    "    combined_fpar.append(selected_fpar)\n",
    "\n",
    "combined_refl = pd.concat(combined_refl)\n",
    "combined_fpar = pd.concat(combined_fpar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sites_list = []\n",
    "i = combined_names.index(\"US-Ha1\")\n",
    "site_name = combined_names[i]\n",
    "site_type = combined_types[i]\n",
    "site_ec = combined_ec[combined_ec[\"name\"] == site_name]\n",
    "site_refl = combined_refl[combined_refl[\"name\"] == site_name]\n",
    "site_fpar = combined_fpar[combined_fpar[\"name\"] == site_name]\n",
    "if site_ec[\"PPFD_IN_QC\"].isna().all():\n",
    "    print(\"No PPFD data for \" + site_name)\n",
    "    bad_sites_list.append(site_name)\n",
    "gpp = site_ec[[\"GPP_NT_VUT_REF\"]]\n",
    "par = site_ec[[\"PPFD_IN\"]]\n",
    "par_qc = site_ec[[\"PPFD_IN_QC\"]]\n",
    "t1 = site_ec[[\"t1\"]]\n",
    "t2 = site_ec[[\"t2\"]]\n",
    "ec_weekly = pd.concat([t1, t2, gpp, par, par_qc], axis=1).rename(\n",
    "    columns={\"GPP_NT_VUT_REF\": \"gpp\", \"PPFD_IN\": \"par\", \"PPFD_IN_QC\": \"par_qc\"}\n",
    ")\n",
    "ec_weekly = ec_weekly[ec_weekly[\"par_qc\"] == 1]\n",
    "ec_weekly = ec_weekly[ec_weekly[\"gpp\"] != -9999]\n",
    "ec_weekly.drop(columns=[\"par_qc\"], inplace=True)\n",
    "ec_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_fpar_filtered = site_fpar[\n",
    "    (site_fpar[\"MCD15A3H_061_FparLai_QC_MODLAND\"] == \"0b0\")\n",
    "    & (site_fpar[\"MCD15A3H_061_FparLai_QC_DeadDetector\"] == \"0b0\")\n",
    "    & (site_fpar[\"MCD15A3H_061_FparLai_QC_CloudState\"] == \"0b00\")\n",
    "    & (site_fpar[\"MCD15A3H_061_FparLai_QC_SCF_QC\"].isin([\"0b000\", \"0b001\"]))\n",
    "]\n",
    "site_fpar_filtered.index = pd.to_datetime(site_fpar_filtered.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_refl_filtered = site_refl[\n",
    "    (\n",
    "        site_refl[\"MCD43A4_061_BRDF_Albedo_Band_Mandatory_Quality_Band1_MODLAND\"]\n",
    "        == \"0b000\"\n",
    "    )\n",
    "    & (\n",
    "        site_refl[\"MCD43A4_061_BRDF_Albedo_Band_Mandatory_Quality_Band2_MODLAND\"]\n",
    "        == \"0b000\"\n",
    "    )\n",
    "]\n",
    "\n",
    "site_refl_filtered.index = pd.to_datetime(site_refl_filtered.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resampled_fpar = []\n",
    "resampled_refl = []\n",
    "\n",
    "for idx, row in ec_weekly.iterrows():\n",
    "    # Select the data for the current week\n",
    "    t1 = pd.to_datetime(row[\"t1\"])\n",
    "    t2 = pd.to_datetime(row[\"t2\"])\n",
    "    mask_fpar = (site_fpar_filtered.index >= t1) & (site_fpar_filtered.index <= t2)\n",
    "    weekly_data_fpar = site_fpar_filtered[mask_fpar]\n",
    "\n",
    "    mask_refl = (site_refl_filtered.index >= t1) & (site_refl_filtered.index <= t2)\n",
    "    weekly_data_refl = site_refl_filtered[mask_refl]\n",
    "\n",
    "    if (weekly_data_fpar.empty) | (weekly_data_refl.empty):\n",
    "        continue\n",
    "\n",
    "    red_tmp = weekly_data_refl[\"MCD43A4_061_Nadir_Reflectance_Band1\"].mean()\n",
    "    nir_tmp = weekly_data_refl[\"MCD43A4_061_Nadir_Reflectance_Band2\"].mean()\n",
    "\n",
    "    tmp_fpar = weekly_data_fpar[\"MCD15A3H_061_Fpar_500m\"]\n",
    "    tmp_fpar_mean = tmp_fpar.mean()\n",
    "    resampled_fpar.append({\"t1\": t1, \"t2\": t2, \"fpar_mean\": tmp_fpar_mean})\n",
    "    resampled_refl.append({\"t1\": t1, \"t2\": t2, \"red\": red_tmp, \"nir\": nir_tmp})\n",
    "\n",
    "resampled_fpar_df = pd.DataFrame(resampled_fpar)\n",
    "resampled_refl_df = pd.DataFrame(resampled_refl)\n",
    "# Select rows from site_ec where t1 and t2 match those in resampled_fpar_df and resampled_refl_df\n",
    "mask = (\n",
    "    ec_weekly[\"t1\"].isin(resampled_fpar_df[\"t1\"])\n",
    "    & ec_weekly[\"t2\"].isin(resampled_fpar_df[\"t2\"])\n",
    ") | (\n",
    "    ec_weekly[\"t1\"].isin(resampled_refl_df[\"t1\"])\n",
    "    & ec_weekly[\"t2\"].isin(resampled_refl_df[\"t2\"])\n",
    ")\n",
    "\n",
    "resampled_ec_df = ec_weekly[mask]  # Note: although called resampled, we dont resample,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First merge resampled_fpar_df and resampled_refl_df\n",
    "merged_df = pd.merge(resampled_fpar_df, resampled_refl_df, how=\"inner\", on=[\"t1\", \"t2\"])\n",
    "\n",
    "# Then merge the result with resampled_ec_df\n",
    "site_df = pd.merge(merged_df, resampled_ec_df, how=\"inner\", on=[\"t1\", \"t2\"]).copy()\n",
    "site_df.loc[:, \"ndvi\"] = (site_df[\"nir\"] - site_df[\"red\"]) / (\n",
    "    site_df[\"nir\"] + site_df[\"red\"]\n",
    ")\n",
    "site_df.loc[:, \"nirv\"] = site_df[\"ndvi\"] * site_df[\"nir\"]\n",
    "site_df.loc[:, \"nirvp\"] = site_df[\"nirv\"] * site_df[\"par\"]\n",
    "site_df.loc[:, \"fesc\"] = site_df[\"nirv\"] / site_df[\"fpar_mean\"]\n",
    "site_df.loc[:, \"lue\"] = site_df[\"gpp\"] / (site_df[\"par\"] * site_df[\"fpar_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df):\n",
    "    # Initialize a boolean mask to keep track of rows to drop\n",
    "    outlier_rows_mask = np.zeros(len(df), dtype=bool)\n",
    "\n",
    "    # Iterate over each column\n",
    "    for col in df.columns:\n",
    "        # Skip the \"t1\" and \"t2\" columns\n",
    "        if col == \"t1\" or col == \"t2\":\n",
    "            continue\n",
    "\n",
    "        # Fit the LocalOutlierFactor model to the column data\n",
    "        lof = LocalOutlierFactor()\n",
    "        outliers = lof.fit_predict(df[col].values.reshape(-1, 1))\n",
    "\n",
    "        # Mark rows with outliers in this column\n",
    "        outlier_rows_mask = np.logical_or(outlier_rows_mask, outliers == -1)\n",
    "\n",
    "    # Drop rows with outliers\n",
    "    cleaned_df = df[~outlier_rows_mask]\n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "# Remove outliers in each column of site_df\n",
    "cleaned_site_df = remove_outliers(site_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(cleaned_site_df[\"fesc\"], cleaned_site_df[\"lue\"])\n",
    "plt.xlabel(\"FESC\")\n",
    "plt.ylabel(\"LUE\")\n",
    "plt.title(\"Harvard Forest LUE vs. FESC\")\n",
    "\n",
    "# Calculate the linear regression and R-squared value\n",
    "slope, intercept, r_value, p_value, std_err = linregress(\n",
    "    cleaned_site_df[\"fesc\"], cleaned_site_df[\"lue\"]\n",
    ")\n",
    "r_squared = r_value**2\n",
    "\n",
    "# Annotate the R-squared value on the plot\n",
    "plt.annotate(f\"R-squared = {r_squared:.2f}\", xy=(0.05, 0.95), xycoords=\"axes fraction\")\n",
    "\n",
    "# Generate x values for the linear line\n",
    "x_fit = np.linspace(cleaned_site_df[\"fesc\"].min(), cleaned_site_df[\"fesc\"].max(), 1000)\n",
    "\n",
    "# Compute the y values of the linear line\n",
    "y_fit = slope * x_fit + intercept\n",
    "\n",
    "# Plot the linear line\n",
    "plt.plot(x_fit, y_fit, \"r-\")\n",
    "\n",
    "# plt.savefig(\"../outputs/figures/weekly_harvard_scatter.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dscovr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
